---
tags:
  - Knowledge
---
## 模型方面
深度学习网络基本结构如下：
- **Backbone**：主干网络，用来提取特征，一般使用resnet、VGG等固定结构；
- **Neck**：融合从backbone不同层提取的特征，进一步提取特征；
- **Head**：利用所提特征来做出预测；
- **Loss**：损失函数，用来衡量预测结果与真实结果的差异。

其它常见结构如下还包括：
- **Bottleneck**：使用$1\times1$的卷积核降低通道数，大幅减少计算量；
- **GAP (Global Average Pooling)**：全局平均池化，将特征图的每个通道的特征值取平均，得到一个特征值，最后将所有通道的特征值拼接起来，作为最终的特征向量；
	![[GAP.jpg|650]]

## 数据方面
### 特征工程
> [!definition] 特征工程 (Feature Engineering)
> 即用对问题的知识和直觉来从已有的原始特征中构造新的特征，来使机器更容易学习。

> [!example] 
> 在预测一块地皮的价格时，我们可以用原有的特征——长与宽，来组合成与地皮价格更加相关的特征——面积，从而提高模型预测的准确率。

### 数据增强
> [!definition] 数据增强 (Data Augmentation)
> 通过变形数据来获取多样性，从而泛化模型，使其鲁棒性更好。同样是一个防止过拟合的方法。

## 训练方面
### 梯度检验
> [!definition] 梯度检验 (Gradient Check)
> 为防止反向传播计算偏导数的过程中出现bug，还可用下式求得梯度近似值进行比较：
> $$\frac{\partial}{\partial\theta_i}J(\Theta)=\frac{1}{2\varepsilon}(J(\theta_1,...,\theta_i+\varepsilon,...,\theta_n)-J(\theta_1,...,\theta_i-\varepsilon,...,\theta_n))$$
> 其中，$\varepsilon$取较小值如$10^{-4}$。将此近似值与用反向传播算出的偏导数比较，若差距过大，则原程序存在bug。

> [!caution] 
> 以此方法求梯度的计算量很大，若已验证程序正常，则要确定关掉梯度检验再进行正式运算，否则程序运行将非常缓慢。

### 丢弃法
> [!definition] 丢弃法 (Dropout)
> 即对隐藏层的输入$x$做如下处理后，将$x'$输入下一层。
> $$
> x_i'=
> \left\{\begin{aligned}
> 0 &,\mathrm{with\ probablity}\ p \\
> \frac{x_i}{1-p}&,\mathrm{otherwise}
> \end{aligned}\right.
> $$

此操作前后$x$期望不变，相当于将隐藏层中的部分神经元丢弃，而去训练一个子网络，最后的模型即是将多个这样训练出来的子网络取平均。之后证明这种方法与正则化相似，但其实际效果要强于正则化。

> [!caution] 
> 神经网络越复杂，丢弃法的影响越大。因此可以将神经网络结构设得复杂一点，再通过丢弃法控制模型复杂度，效果会更好。

### 参数初始化
#### 梯度爆炸与梯度消失
> [!definition] 梯度爆炸与梯度消失
> 对于一$d$层的神经网络$h_t=f_t(h_{t-1})$那么损失$\mathcal{L}$关于第$t$层参数$W_t$的梯度计算则涉及到一连串矩阵的乘法：
> $$\frac{\partial \mathcal{L}}{\partial W_t}=\frac{\partial \mathcal{L}}{\partial h_d}\frac{\partial h_d}{\partial h_{d-1}}...\frac{\partial h_{t+1}}{\partial h_t}\frac{\partial h_t}{\partial W_t}$$
> - **梯度爆炸**：此处若这些矩阵的元素大于1，积累到一定次数，梯度将超出值域，这种现象称为梯度爆炸；
> - **梯度消失**：同理，若矩阵元素小于1，积累到一定次数，梯度将归于0，这种现象称为梯度消失。

- 梯度爆炸的问题：对学习率敏感，学习率太大则加速爆炸，太小则训练无进展；
- 梯度消失的问题：学习无进展，尤其是底部层，因此无法使神经网络更深。

对于此问题，有以下几种解决方法：
1. **选择合适的激活函数**：要保持均值与方差不变，则激活函数需要满足其在零点处的切线斜率为1，截距为0。下图中除了sigmoid函数，其它几种函数都符合此条件：
	![[sigma.png|250]]
1. **梯度裁剪**：若梯度的模长大于某个值$\theta$，则通过投影将其模长拖回$\theta$。此方法可有效地预防梯度爆炸。
2. **选择合适的初始化**。

#### 合适的初始化策略
> [!definition] Xavier初始化 (Xavier Initialization)
> Xavier初始化可令参数按照正态分布$\mathcal{N}\left(0,\sqrt{\frac{2}{n_{t-1}+n_t}}\right)$生成。

我们设$h_{t-1}$独立于$W_t$，且令
$$
\left\{\begin{aligned}
E(W_t)&=0\\
D(W_t)&=\gamma_t
\end{aligned}\right.
$$
其中$\gamma_t$为常数。可求出$h_t$的正向方差$D(h_t)$与反向梯度的方差$D\left(\frac{\partial \mathcal{L}}{\partial h_{t-1}}\right)$为
$$
\left\{\begin{aligned}
D(h_t)=n_{t-1}\gamma_tD(h_{t-1})\\
D\left(\frac{\partial \mathcal{L}}{\partial h_{t-1}}\right)=n_t\gamma_tD\left(\frac{\partial \mathcal{L}}{\partial h_t}\right)
\end{aligned}\right.
$$
要保持方差不变，则需$n_{t-1}\gamma_t=n_t\gamma_t=1$，但此条件难以满足。此时我们可以使
$$\gamma_t=\frac{2}{n_{t-1}+n_t}$$
（即Xavier初始化），这样就使得$\gamma_t$能够尽量贴近于上述条件。

#### 迁移学习
已经训练好的参数同样可以用来初始化模型。

一个神经网络一般可以分为两个部分，底层部分是用来提取特征的，顶层的全连接部分是用来总结分析特征来分类模型的。不同模型可能需要分类的任务不一样，但提取的特征是可以重复使用的。

> [!definition] 迁移学习 (Transfer Learning)
> 迁移学习即是用大数据训练好的复杂模型的参数对某个特定任务模型来进行初始化，来简化特定任务模型的训练，并提高此模型的准确性。其中，用来初始化的模型称为**预训练模型(Pre-training)**，训练预训练模型的大数据集称为**源数据集**，用小规模数据继续在预训练模型上训练的操作称为**微调(Fine Tuning)**。

> [!caution] 
> 源数据集要远复杂于目标数据，所以为避免模型在目标数据集上过拟合，一般会使用更强的正则化，通常通过以下手段实现：
> - 使用更小的学习率；
> - 迭代更少的次数；
> - 由于低层次的特征更加通用，有时甚至可以固定住底层的参数不参与训练。

迁移学习省去了模型训练冷启动的时间，通常速度更快，精度更高。

### 批量
#### 批量处理
批量处理数据可以允许模型的并行计算，从而加快训练速度，但批量大小的选择也会影响模型的性能，因此批量大小也会作为一个超参数来选择。通常来说，批量越大：
- 每次迭代所需的时间越短；
- 每次迭代损失减少得越慢；
- 模型的泛化能力越差。

总体来说，批量大小越大，模型的收敛速度越慢，且模型的泛化能力越差。这主要是因为小批量所找到的最小值更加平坦，且大批量找到的最小值会离初始权重较近。然而，可以用增大学习率的方式来增加收敛速度，但其性能依然不及小批量。

#### 批量归一化
[[机器学习技巧#梯度爆炸与梯度消失|上面]]给出的三种避免梯度爆炸与梯度消失的方法，其本质其实都是去约束每一层的分布，使得神经网络每一层的输出不至于跑得太偏。参考简单的机器学习任务中，为防止收敛速度过慢，输入一般都需要进行归一化处理。由此，我们可以在网络中的每一层都加上一步归一化，来约束其输出。

但若仅仅在每层添加归一化步骤，当神经网络很深时，由于损失的计算在最后，反向传播时顶部的层训练很快，底部的层则训练较慢，但底层一变化，顶层就得根据新的顶层重复学习，收敛速度同样不高。因此我们需要添加额外的参数，及时调整每层数据的分布。

> [!definition] 批量归一化 (Batch Normalization)
> 取一批数据在第$d$层的输出$x_{id}$均值$\mu_d$与方差$\sigma_d^2$，归一化后得出$\hat{x}_{id}$：
> $$\hat{x}_{id}=\frac{x_{id}-\mu_d}{\sigma_d}$$
> 添加额外的参数$\beta_d,\gamma_d$，得到最终的输出$y_{id}$：
> $$y_{id}=\gamma_{d}\hat{x}_{id}+\beta_d$$
> 模型训练过程中我们会不断地更新$\mu_{d}$与$\sigma_{d}^2$，但在测试阶段会固定这两个量。

此方法允许使用更高的学习率，从而加速模型收敛，但不会改变模型精度。

> [!note] 
> 有论文指出批量归一化就是通过在每个小批量里加入噪音来控制模型复杂度，因此不必与丢弃法混用。
