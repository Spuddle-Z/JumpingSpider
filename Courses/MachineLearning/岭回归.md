---
tags:
  - Knowledge
aliases:
  - Ridge Regression
---
## 岭回归
令$X$为$n$行$p$列的矩阵，表示$n$个$p$维向量的集合，则岭回归的损失函数为
$$l(\beta)=||Y-X\beta||^2+\lambda||\beta||^2$$
由于此函数为凸函数，因此其最小化问题$\min_{\beta}l(\beta)$为一个凸优化，其[[凸优化#KKT条件|KKT条件]]中的稳定性条件即可得到其最优解，即解
$$
\begin{aligned}
&\frac{\partial l}{\partial \beta}=-2X^T(Y-X\beta)+2\lambda\beta=0\\
\implies&\hat{\beta}_{\lambda}=(X^TX+\lambda I)^{-1}X^TY
\end{aligned}
$$
![[Pasted image 20240419101910.png|475]]
由图中可以看到，正则化项$\lambda||\beta||^2$惩罚了那些有权重上有绝对优势的特征，使得权重的分布更加均匀，不让鸡蛋都放在一个篮子里，增强了模型的鲁棒性。
## 
#broken 4.9-2-23
$$$$
## 核回归
- **核回归(Kernel Regression)**：在原岭回归的基础上使用[[SVM#核方法|核方法]]，将原特征$X_{n\times p}$变为一个核$K_{n\times n}$，则原岭回归变为
	$$
	\begin{aligned}
	&\min_{c}\sum_{i=1}^{n}\left\lVert y_i-\sum_{j=1}^{n}c_jK_{ij}\right\rVert^2+\lambda\sum_{i,j}c_ic_jK_{ij}\\
	=&\min_{c}\left\lVert Y-Kc\right\rVert^2+\lambda c^TKc
	\end{aligned}
	$$

同样求解其稳定性条件
$$\frac{\partial l}{\partial c}=0\implies\hat{c}_{\lambda}=(K+\lambda I)^{-1}Y$$
此处正则化项$\lambda c^TKc$同样起到均匀权重分布的作用。

从岭回归的视角观察核回归，可以发现核回归的损失函数为
$$\left\lVert Y-\phi(X)\beta\right\rVert^2+\lambda\left\lVert\beta\right\rVert^2$$
其中$\beta=\sum_ic_i\phi(x_i)$，将其代入后可以看到，此函数就是核回归的损失函数。因此，核回归的本质也是一个岭回归。
## 样条回归
![[Pasted image 20240419113527.png|250]]
- **样条回归(Spline Regression)**：通过分段线性模型来进行拟合。其损失函数如下
	$$\sum_{i=1}^{n}\left\lVert y_i-\alpha_0-\sum_{j=1}^{p}\alpha_j\max(0,x_i-k_j)\right\rVert^2+\lambda\sum_{j=1}^{p}\alpha_j^2$$

将其改写为矩阵的表达形式
$$l(\alpha)=\left\lVert Y-Z\alpha\right\rVert^2+\lambda\left\lVert D\alpha\right\rVert^2$$
其中
$$
\left\{\begin{aligned}
Z&=
\begin{bmatrix}
1&\max(0,x_1-k_1)&\cdots&\max(0,x_1-k_p)\\
1&\max(0,x_2-k_1)&\cdots&\max(0,x_2-k_p)\\
\vdots&\vdots&\ddots&\vdots\\
1&\max(0,x_n-k_1)&\cdots&\max(0,x_n-k_p)
\end{bmatrix}\\
\alpha&=[\alpha_0,\alpha_1,...,\alpha_p]^T\\
D&=\text{diag}(0,1,...,1)
\end{aligned}\right.
$$
与岭回归和核回归同理可得
$$\hat{\alpha}_{\lambda}=(Z^TZ\lambda D)^{-1}Z^TY$$
> [!note] 
> 神经网络的本质也是一个样条回归。为神经网络引入非线性特征的是ReLU函数，其本质就是一个$\max(0,x+1)$，因此神经网络所拟合出的模型其实是由一个个小的线性部分拼接而成的。

