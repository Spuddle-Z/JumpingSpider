---
tags:
  - Knowledge
aliases:
  - Ridge Regression
---
## 岭回归
> [!definition] 岭回归 (Ridge Regression)
> 形式上就是线性回归加上一个正则化项$\lambda||\beta||^2$。令$X$为$n$行$p$列的矩阵，表示$n$个$p$维向量的集合，则岭回归的损失函数为
> $$l(\beta)=||Y-X\beta||^2+\lambda||\beta||^2$$

由于其损失函数为凸函数，因此其最小化问题$\min_{\beta}l(\beta)$为一个凸优化，解出其[[凸优化#KKT条件|KKT条件]]中的稳定性条件即可得到其最优解，即解
$$
\begin{aligned}
&\frac{\partial l}{\partial \beta}=-2X^T(Y-X\beta)+2\lambda\beta=0\\
\implies&\hat{\beta}_{\lambda}=(X^TX+\lambda I)^{-1}X^TY
\end{aligned}
$$
![[Pasted image 20240419101910.png|475]]
由图中可以看到，正则化项$\lambda||\beta||^2$惩罚了那些有权重上有绝对优势的特征，使得权重的分布更加均匀，不让鸡蛋都放在一个篮子里，增强了模型的鲁棒性。
## 核回归
> [!definition] 核回归 (Kernel Regression)
> 在原岭回归的基础上使用[[SVM#核方法|核方法]]，将原特征$X_{n\times p}$变为一个核$K_{n\times n}$，则原岭回归变为
> $$
> \begin{aligned}
> &\min_{c}\sum_{i=1}^{n}\left\lVert y_i-\sum_{j=1}^{n}c_jK_{ij}\right\rVert^2+\lambda\sum_{i,j}c_ic_jK_{ij}\\
> =&\min_{c}\left\lVert Y-Kc\right\rVert^2+\lambda c^TKc
> \end{aligned}
> $$

同样求解其稳定性条件
$$\frac{\partial l}{\partial c}=0\implies\hat{c}_{\lambda}=(K+\lambda I)^{-1}Y$$
此处正则化项$\lambda c^TKc$同样起到均匀权重分布的作用。

从岭回归的视角观察核回归，可以发现核回归的损失函数为
$$\left\lVert Y-\phi(X)\beta\right\rVert^2+\lambda\left\lVert\beta\right\rVert^2$$
其中$\beta=\phi^T(X)c$，将其代入后可以看到，此函数就是核回归的损失函数。因此，核回归的本质也是一个岭回归。
## 贝叶斯回归
假设
$$
\left\{\begin{aligned}
&\beta\sim N(0,\tau^2I_{p})\\
&\varepsilon\sim N(0,\sigma^2I_n)
\end{aligned}\right.
$$
则在所有样本下，$\beta$的最大似然估计为
$$P(\beta|X,Y)=\frac{P(\beta|X)P(Y|X\beta)}{P(Y|X)}$$
我们分别看这三部分：
- $P(Y|X)$是固定的；
- 由于$Y=X\beta+\varepsilon$，因此$P(Y|X\beta)\sim P(\varepsilon)$；
- 当$Y$不确定，只有$X$没法控制住$\beta$，所以$P(\beta|X)\sim P(\beta)$。

因此可以认为
$$
\begin{aligned}
P(\beta|X,Y)&\varpropto P(\beta)P(Y|X\beta)\\
&\varpropto\exp\left( -\frac{1}{2\tau^2}|\beta|^2 \right)\exp\left( -\frac{1}{2\sigma^2}|Y-X\beta|^2 \right)\\
&=\exp\left( -\frac{1}{2}\left[ \frac{1}{\sigma^2}|Y-X\beta|^2+\frac{1}{\tau^2}|\beta|^2 \right] \right)
\end{aligned}
$$
因此
$$\mathop{\arg\!\max}\limits_{\beta}\ P(\beta|Y,X)=\mathop{\arg\!\min}\limits_{\beta}\ |Y-X\beta|^2+\frac{\sigma^2}{\tau^2}|\beta|^2$$
可以看到等式右侧如果把$\frac{\sigma^2}{\tau^2}$换成$\lambda$就是岭回归，即*岭回归的本质是对于$\beta$的最大似然估计*。
## 高斯过程
如果固定$X$，则有
$$
\left\{\begin{aligned}
E(Y)&=XE(\beta)+E(\varepsilon)=0\\
D(Y)&=XD(\beta)X^T+D(\varepsilon)=\tau^2XX^T+\sigma^2I_n
\end{aligned}\right.
\implies Y\sim N(0,\tau^2XX^T+\sigma^2I_n)
$$
再求$Y,\beta$的协方差
$$
\begin{aligned}
\sigma(Y,\beta)&=E(Y\beta^T)+E(Y)E(\beta)\\
&=E(X\beta\beta^T)+E(\varepsilon\beta^T)\\
&=X\tau^2I_{p}+0\\
&=\tau^2X
\end{aligned}
$$
因此
$$
\begin{bmatrix}
Y\\\beta
\end{bmatrix}
\sim N\left(
\begin{bmatrix}0\\0\end{bmatrix},
\begin{bmatrix}
\tau^2XX^T+\sigma^2I_n&\tau^2X\\
\tau^2X^T&\tau^2I_{p}
\end{bmatrix}
\right)
$$
此时可以求得$P(\beta|X,Y)=P(\beta|Y)$遵循
$$N(\tau^2X^T(\tau^2XX^T+\sigma^2I_n)^{-1}Y,\tau^2I_{p}-\tau^2X^T(\tau^2XX^T+\sigma^2I_n)^{-1}\tau^2X)$$
