---
tags:
  - Knowledge
aliases:
  - Ridge Regression
---
## 岭回归
> [!definition] 岭回归 (Ridge Regression)
> 形式上就是线性回归加上一个正则化项$\lambda||\beta||^2$。令$X$为$n$行$p$列的矩阵，表示$n$个$p$维向量的集合，则岭回归的损失函数为
> $$l(\beta)=||Y-X\beta||^2+\lambda||\beta||^2$$

由于其损失函数为凸函数，因此其最小化问题$\min_{\beta}l(\beta)$为一个凸优化，解出其[[凸优化#KKT条件|KKT条件]]中的稳定性条件即可得到其最优解，即解
$$
\begin{aligned}
&\frac{\partial l}{\partial \beta}=-2X^T(Y-X\beta)+2\lambda\beta=0\\
\implies&\hat{\beta}_{\lambda}=(X^TX+\lambda I)^{-1}X^TY
\end{aligned}
$$
![[Pasted image 20240419101910.png|475]]
由图中可以看到，正则化项$\lambda||\beta||^2$惩罚了那些有权重上有绝对优势的特征，使得权重的分布更加均匀，不让鸡蛋都放在一个篮子里，增强了模型的鲁棒性。
## 核回归
> [!definition] 核回归 (Kernel Regression)
> 在原岭回归的基础上使用[[SVM#核方法|核方法]]，将原特征$X_{n\times p}$变为一个核$K_{n\times n}$，则原岭回归变为
> $$
> \begin{aligned}
> &\min_{c}\sum_{i=1}^{n}\left\lVert y_i-\sum_{j=1}^{n}c_jK_{ij}\right\rVert^2+\lambda\sum_{i,j}c_ic_jK_{ij}\\
> =&\min_{c}\left\lVert Y-Kc\right\rVert^2+\lambda c^TKc
> \end{aligned}
> $$

同样求解其稳定性条件
$$\frac{\partial l}{\partial c}=0\implies\hat{c}_{\lambda}=(K+\lambda I)^{-1}Y$$
此处正则化项$\lambda c^TKc$同样起到均匀权重分布的作用。

从岭回归的视角观察核回归，可以发现核回归的损失函数为
$$\left\lVert Y-\phi(X)\beta\right\rVert^2+\lambda\left\lVert\beta\right\rVert^2$$
其中$\beta=\phi^T(X)c$，将其代入后可以看到，此函数就是核回归的损失函数。因此，核回归的本质也是一个岭回归。
## 贝叶斯回归
假设
$$
\left\{\begin{aligned}
&\beta\sim N(0,\tau^2I_{p})\\
&\varepsilon\sim N(0,\sigma^2I_n)
\end{aligned}\right.
$$
则在所有样本下，$\beta$的最大似然估计为
$$P(\beta|X,Y)=\frac{P(\beta|X)P(Y|X,\beta)}{P(Y|X)}$$
我们分别看这三部分：
- $P(Y|X)$是固定的；
- 由于$Y=X\beta+\varepsilon$，因此$P(Y|X\beta)\sim P(\varepsilon)$；
- 当$Y$不确定，只有$X$没法控制住$\beta$，所以$P(\beta|X)\sim P(\beta)$。

因此可以认为
$$
\begin{aligned}
P(\beta|X,Y)&\varpropto P(\beta)P(Y|X\beta)\\
&\varpropto\exp\left( -\frac{1}{2\tau^2}|\beta|^2 \right)\exp\left( -\frac{1}{2\sigma^2}|Y-X\beta|^2 \right)\\
&=\exp\left( -\frac{1}{2}\left[ \frac{1}{\sigma^2}|Y-X\beta|^2+\frac{1}{\tau^2}|\beta|^2 \right] \right)
\end{aligned}
$$
因此
$$\mathop{\arg\!\max}\limits_{\beta}\ P(\beta|Y,X)=\mathop{\arg\!\min}\limits_{\beta}\ |Y-X\beta|^2+\frac{\sigma^2}{\tau^2}|\beta|^2$$
可以看到等式右侧如果把$\frac{\sigma^2}{\tau^2}$换成$\lambda$就是岭回归，即*岭回归的本质是对于$\beta$的最大似然估计*。
## 高斯过程
用另一种方法推导$P(\beta|X,Y)$，同样可以得到与岭回归和贝叶斯回归相同的结果。我们将$X$看作常值，则有$P(\beta|X,Y)=P(\beta|Y)$。根据[[概率论#运算规律|期望与方差的计算公式]]，有
$$
\left\{\begin{aligned}
E(Y)&=XE(\beta)+E(\varepsilon)=0\\
D(Y)&=XD(\beta)X^T+D(\varepsilon)=\tau^2XX^T+\sigma^2I_n
\end{aligned}\right.
\implies Y\sim N(0,\tau^2XX^T+\sigma^2I_n)
$$
再求$Y,\beta$的协方差
$$
\begin{aligned}
\sigma(Y,\beta)&=E(Y\beta^T)+E(Y)E(\beta)\\
&=E(X\beta\beta^T)+E(\varepsilon\beta^T)\\
&=X\tau^2I_{p}+0\\
&=\tau^2X
\end{aligned}
$$
因此
$$
\begin{bmatrix}
Y\\\beta
\end{bmatrix}
\sim N\left(
\begin{bmatrix}0\\0\end{bmatrix},
\begin{bmatrix}
\tau^2XX^T+\sigma^2I_n&\tau^2X\\
\tau^2X^T&\tau^2I_{p}
\end{bmatrix}
\right)
$$
此时可以求得$P(\beta|Y)$遵循
$$N(\tau^2X^T(\tau^2XX^T+\sigma^2I_n)^{-1}Y,\tau^2I_{p}-\tau^2X^T(\tau^2XX^T+\sigma^2I_n)^{-1}\tau^2X)$$
即
$$\hat{\beta}=\tau^2X^T(\tau^2XX^T+\sigma^2I_n)^{-1}Y$$
> [!note] 
> 可以证明岭回归的最优解$\hat{\beta}_{\lambda}=(X^TX+\lambda I)^{-1}X^TY$与高斯过程求出的$\hat{\beta}$等价。
> > [!proof] 
> > 由于$\lambda\geq0$为可调一参数，$\sigma^2$与$\varepsilon$有关，$\tau^2$只与$\beta$有关，同样可调，则可令$\lambda=\frac{\sigma^2}{\tau^2}$，有
> > $$
> > \begin{aligned}
> > \hat{\beta}&=\tau^2X^T(\tau^2XX^T+\sigma^2I_n)^{-1}Y\\
> > &=X^T\left( XX^T+\frac{\sigma^2}{\tau^2}I_n \right)^{-1}Y\\
> > &=X^T(XX^T+\lambda I_n)^{-1}Y
> > \end{aligned}
> > $$
> > 又因为
> > $$X^T(XX^T+\lambda I_n)=X^TXX^T+\lambda X^T=(X^TX+\lambda I_{p})X^T$$
> > 等式两侧同时左乘$(X^TX+\lambda I_{p})^{-1}$并右乘$(XX^T+\lambda I_n)^{-1}Y$，得
> > $$(X^TX+\lambda I_{p})^{-1}X^TY=X^T(XX^T+\lambda I_n)^{-1}Y$$

使用核方法后，也同样可以得到
$$
\left\{\begin{aligned}
Y&\sim N(0,\tau^2K+\sigma^2I_n)\\
\begin{bmatrix}
Y\\c(x_0)
\end{bmatrix}
&\sim N\left(
\begin{bmatrix}0\\0\end{bmatrix},
\begin{bmatrix}
K+\sigma^2I_n&K(x,x_0)\\
K(x_0,x)&K(x_0,x_0)
\end{bmatrix}_{(n+1)\times(n+1)}
\right)
\end{aligned}\right.
$$
其中
$$
\left\{\begin{aligned}
K_{ij}&=K(X_i,X_j)=\phi(X_i)^T\phi(X_j)\\
c(x_0)&=\phi(x_0)^T\beta
\end{aligned}\right.
$$
则有$P(c(x_0)|X,Y)$遵循
$$N(K(x_0,x)(K+\sigma^2I_n)^{-1}Y,K(x_0,x_0)-K(x_0,x)(K+\sigma^2I_n)^{-1}K(x,x_0))$$
