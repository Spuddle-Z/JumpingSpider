---
tags:
  - Knowledge
aliases:
  - Principal Component Analysis
  - 主成分分析
---
## 基本概念
数据维数太多不便于分析，我们通过去掉一些无关的特征，保留数据变化最剧烈的方向，来实现数据的降维。降维的过程意味着我们总要损失一些信息，因此我们希望损失的信息尽可能地小。
> [!definition] 主成分分析 (Principal Component Analysis, PCA)
> ![[Pasted image 20240514104405.png|275]]
> 设原数据$X$为$n\times p$的矩阵，即$n$个$p$维向量，假设我们要将其降到$k$维，我们即需要右乘一个$p\times k$的矩阵$W$，得到降维后的数据$T$
> 1. 平移原数据，令其均值为零，得到处理后的数据$X$；
> 2. 求$X^TX$最大特征值对应的特征向量$w_{(1)}$；
> 3. 求得忽略$w_{(1)}$方向后的数据$X'=X-Xw_{(1)}w_{(1)}^T$；
> 4. 再求$X'^TX'$最大特征值对应的特征向量$w_{(2)}$，之后按照此方法求得$w_{(k)}$；
> 5. 将这$k$个列向量合并为矩阵$W$，可得
> $$T=XW$$
## 数学推导
为了尽可能保留原数据间差异最大的特征，我们需要找到原数据在某一个方向$w_1$上的投影，并使其尽量分散，即方差最大。此时首先平移原数据，令其均值为零，便于求其方差。

其数学表达如下：
$$w_{(1)}=\mathop{\arg\!\max}\limits_{||w_{(1)}||=1}\ \left( \sum_i||X_iw_{(1)}||^2 \right)=\mathop{\arg\!\max}\limits_{||w_{(1)}||=1}\ (w_{(1)}^TX^TXw_{(1)})$$
将上式的形式稍作整理，并将$w_{(1)}$为单位向量的条件加入到后式中，得到
$$w_{(1)}=\mathop{\arg\!\max}\limits_{w_{(1)}}\ \left( \frac{w_{(1)}^TX^TXw_{(1)}}{w_{(1)}^Tw_{(1)}} \right)$$
对此式使用[[凸优化#拉格朗日乘子法|拉格朗日乘子法]]，得到
$$\frac{\partial }{\partial w_{(1)}}[w_{(1)}^TX^TXw_{(1)}-\lambda(w_{(1)}^Tw_{(1)}-1)]\implies X^TXw_{(1)}=\lambda w_{(1)}$$
即$w_{(1)}$为$X^TX$最大特征值所对应的特征向量。

此时若想求方差第二大的方向，我们需要忽略$w_{(1)}$方向上的数值
$$X'=X-Xw_{(1)}w_{(1)}^T$$
再对$X'$做一遍求最大方差方向的步骤，即$w_{(2)}$为$X'^TX'$最大特征值所对应的特征向量，之后的步骤以此类推。
