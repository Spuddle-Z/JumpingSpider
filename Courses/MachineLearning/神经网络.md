---
tags:
  - Knowledge
aliases:
  - Neural Network
---
## 基本概念
> [!note] 
![[Pasted image 20240501223355.png|300]]
神经网络的本质其实也是一个对空间映射之后的线性分类。

![[Pasted image 20240502212048.png|225]]
**符号表示**：$s_l$表示第$l$层线性变换后的结果，$W_l,b_l$表示第$l$层线性变换的参数，$h_l$表示第$l$层的输出，$f$为激活函数，即
$$
\left\{\begin{aligned}
s_l&=W_lh_{l-1}+b_l\\
h_l&=f(s_l)
\end{aligned}\right.
$$
其中$h_0=X$，我们可以由上式一步步推出输出$h_{L}$，并由$h_{L}$得到损失函数$L(Y,h_{L})$。
> [!note] 
> $f$这一步非线性变化也可以用矩阵表示出来，即
> $$
> f(s_l)=f_ls_l
> \begin{bmatrix}
> \mathbb{1}(s_{l1})&0&\cdots&0\\
> 0&\mathbb{1}(s_{l2})&\cdots&0\\
> \vdots&\vdots&\ddots&\vdots\\
> 0&0&\cdots&\mathbb{1}(s_{lk})
> \end{bmatrix}s_l
> $$
> 可以看到方阵$f_l$是一个与$s_l$有关的函数，那么当输入$h_0$在一个足够小的范围内变化时，$s_l$也会在一个极小的范围内变化，这就使得$f_l$是一个固定的矩阵，那么$h_0$和$h_{L}$之间就退化成了线性变化。这也可以说明，*神经网络的本质是众多的局部线性片段*。

> [!definition] 激活函数 (Activation Function)
> 为神经网络引入非线性的运算。常用激活函数有ReLU、Sigmoid、tanh等。

Sigmoid函数有两个缺点：
- 在输入较大时，其梯度十分接近于零，在反向传播时会造成*梯度消失*；
- 其计算较为复杂。

可引入ReLU函数解决以上两个问题。
> [!definition] 线性整流单元 (Rectified Linear Unit, ReLU)
> $$ReLU(x)=max(0,x)$$

> [!caution] 
> 输出层要输出概率，不能用ReLU替换。
## 反向传播
> [!definition] 反向传播 (Backpropagation)
> 指损失函数对参数的梯度通过反向计算得到。

参考[[线性代数#矩阵的求导运算|向量的求导法则]]，具体计算公式如下：
$$
\begin{aligned}
\frac{\partial L}{\partial h_{l-1}}&=\left(\frac{\partial L}{\partial h_{l-1}^T}\right)^T\\
&=\left(\frac{\partial L}{\partial h_l^T}\frac{\partial h_l}{\partial s_l^T}\frac{\partial s_l}{\partial h_{l-1}^T}\right)^T\\
&=\left(\frac{\partial L}{\partial h_l^T}f_l'W_l\right)^T\\
&=W_l^Tf_l'\frac{\partial L}{\partial h_l}
\end{aligned}
$$
其中
$$
f_l'=\frac{\partial h_l}{\partial s_l^T}=
\begin{bmatrix}
\frac{\partial h_{l1}}{\partial s_{l1}}&0&\cdots&0\\
0&\frac{\partial h_{l2}}{\partial s_{l2}}&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\frac{\partial h_{lk}}{\partial s_{lk}}
\end{bmatrix}
$$
我们定义
$$
\left\{\begin{aligned}
\Delta h_l&=\frac{\partial L}{\partial h_l}\\
\Delta W_l&=\frac{\partial L}{\partial W_l}\\
D_l&=f_l'
\end{aligned}\right.
$$
则有
$$
\left\{\begin{aligned}
\Delta h_{l-1}&=W_l^TD_l\Delta h_l\\
\Delta W_l&=D_l\Delta h_lh_{l-1}^T\\
\Delta b_l&=D_l\Delta h_l
\end{aligned}\right.
$$
$W_l,b_l$均为需要学习的参数，我们统称为$\theta$。
## 梯度下降
### 基本概念
定义$L_i(\theta)=L(X_i,Y_i,\theta)$为第$i$个数据经过参数为$\theta$的网络产生的损失，再设$\mathcal{L}(\theta)=\frac{1}{n}\sum_{i=1}^{n}L_i(\theta)$为整个训练集损失的平均值。
> [!definition] 梯度下降法 (Gradient Descent)
> 通过下面的公式来更新参数$\theta$，训练出更好的模型参数
> $$\theta_t=\theta_{t-1}-\eta\frac{\partial \mathcal{L}}{\partial \theta}$$
> 其中$\eta$为学习率，控制步长大小；$\theta_t$表示第$t$次迭代后的参数。
### 随机梯度下降
上述的梯度下降法需要计算所有的数据，计算量较大，并且容易陷入局部最小值。为解决这两个问题，我们引入随机梯度下降：
> [!definition] 随机梯度下降法 (Stochastic gradient descent, SGD)
> 随机选择一个小批量的样本，然后用这个小批量样本的平均梯度来更新参数。

对于全局的数据来说，这个小批量样本产生的梯度是有噪音的，这个随机的噪音就令其有机会逃出局部最小值。
### Momentum, Adagrad, RMSprop, Adam
使用SGD经常会出现下面左图所示的情况：
![[Pasted image 20240503192629.png|270]]
图中的红色箭头是最快速的下降路径，但梯度的方向却常常如黑色箭头一样，这就使得在下降过程中出现振荡的情况。有几种方法能够缓解这一现象。

> [!definition] 动量法 (Momentum)
> 使用如下的式子更新参数，能起到一个平滑路径的效果
> $$
> \left\{\begin{aligned}
> v_t&=\gamma v_{t-1}+\eta g_t\\
> \theta_t&=\theta_{t-1}-v_t
> \end{aligned}\right.
> $$
> 其中$g$是小批量的平均损失，$v$称作**动量(Momentum)**或**速率(Velocity)**，$\gamma$为超参数，通常设为0.9。

> [!definition] 自适应梯度法 (Adaptive Gradient Algorithm, AdaGrad)
> 使用下式更新参数
> $$
> \left\{\begin{aligned}
> G_t&=G_{t-1}+g_t^2\\
> \theta_{t+1}&=\theta_{t}-\eta\frac{g_t}{\sqrt{G_t+\varepsilon}}
> \end{aligned}\right.
> $$
> $\varepsilon$是一个很小的值，纯粹是用来避免除零错误的。此算法相当于动态调整学习率，使其随迭代慢慢变小。

> [!definition] 方根传播 (Root Mean Square Propagation, RMSprop)
> 使用下式更新参数
> $$
> \left\{\begin{aligned}
> G_t&=\beta G_{t-1}+(1-\beta)g_t^2\\
> \theta_{t+1}&=\theta_{t}-\eta\frac{g_t}{\sqrt{G_t+\varepsilon}}
> \end{aligned}\right.
> $$
> 此方法与Adagrad相似，同样使学习率慢慢变小，只是$G_t$的求法有所变化。

> [!definition] 自适应矩估计 (Adaptive Moment Estimation, Adam)
> 其从梯度方向与学习率两方面入手，综合了momentum和RMSprop的优点。
> $$
> \left\{\begin{aligned}
> v_t&=\gamma v_{t-1}+(1-\gamma)g_t\\
> \hat{v_t}&=\frac{v_t}{1-\gamma}\\
> G_t&=\beta G_{t-1}+(1-\beta)g_t^2\\
> \hat{G_t}&=\frac{G_t}{1-\beta}\\
> \theta_{t+1}&=\theta_t-\eta\frac{\hat{v}_t}{\sqrt{\hat{G}_t+\varepsilon}}
> \end{aligned}\right.
> $$