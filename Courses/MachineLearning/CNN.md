---
tags:
  - Knowledge
aliases:
  - Convolutional Neural Networks
  - 卷积神经网络
---
## 背景
对于图片识别来说，如果将一张图片按每个像素点拉成一个向量，把这个向量作为神经网络的输入，则整个神经网络需要极其巨大的参数量，要去训练这个模型几乎是不可能的，并且图片识别的两个特性说明这个输入向量的一些特征是不重要的。这两个特性是**平移不变性**与**局部性**。

- 平移不变性：目标位置的改变并不影响其识别；
- 局部性：识别目标不需要看到图片的整体。

问题中这两个特性的存在，允许我们可以适当削减模型的复杂度，减少训练代价并获得同样的效果。

## 基础CNN
![[Pasted image 20240503223729.png|650]]
此时引入卷积层，这是一个特殊的全连接层，利用卷积核的平移采集局部特征。这样就使得卷积核内的参数可以被重复使用，大大减少了参数数量。

得到卷积核的过程同样利用机器学习的思路，给定输入$X$与输出$Y$，通过梯度下降最小化损失函数。

实际图片中，我们想要的特征可能会发生小幅平移或旋转，如果卷积层对位置太敏感，则会降低模型效果。
> [!definition] 池化 (Pooling)
> 将窗口内的所有值缩减为一个值，在压缩特征图的同时，保留目标特征，降低卷积层对位置的敏感性。
> - **最大池化(Max Pooling)**：保留窗口内的最大值；
> - **平均池化(Average Pooling)**：保留窗口内的平均值。

> [!caution] 
> *Softmax层之前不能接ReLU层。*
> 
> ReLU层的输出最小也是零，指数运算后最小就为1。我们知道在分类问题中，softmax输出的是概率，对于无关的选项，我们希望其概率为零。但此时此选项分子最小值就为1，那么分母就必须非常大，才能符合我们的预期。
## AlexNet、VGG、NiN与Inception
> [!definition] AlexNet
> ![[Pasted image 20240504154320.png|600]]

AlexNet优化了一些细节：
- 将激活函数从Sigmoid变成ReLU；
- 将平均池化改为最大池化。

> [!definition] VGG
> VGG有两个版本，即VGG16与VGG19。下图为VGG16的结构。
> ![[Pasted image 20240504154440.png|545]]

VGG则引入了**VGG块(VGG Block)**的概念。将数个3×3卷积层与一个2×2的池化层组合成一个VGG块，通过堆叠这样的VGG块来得到更大更深也更规整的CNN。

NiN则通过用1×1的卷积层替代卷积层后的第一个全连接层。这个操作相当于将每个通道看作一个元素，做了一个通道之间的全连接层，而非把通道内向量中的每一个数看作是一个元素，这样极大地减少了这一层的参数量，同时也有效地削减了维度。

GoogLeNet引入了inception块。
> [!definition] Inception块
> ![[inception.png|475]]
> Inception块中每一条平行路径都是一个通道，输出则综合了4个通道的结果。与3×3的单层卷积层相比，inception块反而有更少的参数量与复杂度，却融合了更多的结构。
## 梯度爆炸与梯度消失
> [!definition] 梯度爆炸与梯度消失
> 对于一$d$层的神经网络$h_t=f_t(h_{t-1})$那么损失$\mathcal{L}$关于第$t$层参数$W_t$的梯度计算则涉及到一连串矩阵的乘法：
> $$\frac{\partial \mathcal{L}}{\partial W_t}=\frac{\partial \mathcal{L}}{\partial h_d}\frac{\partial h_d}{\partial h_{d-1}}...\frac{\partial h_{t+1}}{\partial h_t}\frac{\partial h_t}{\partial W_t}$$
> - **梯度爆炸**：此处若这些矩阵的元素大于1，积累到一定次数，梯度将超出值域，这种现象称为梯度爆炸；
> - **梯度消失**：同理，若矩阵元素小于1，积累到一定次数，梯度将归于0，这种现象称为梯度消失。

- 梯度爆炸的问题：对学习率敏感，学习率太大则加速爆炸，太小则训练无进展；
- 梯度消失的问题：学习无进展，尤其是底部层，因此无法使神经网络更深。

对于此问题，有以下几种解决方法：
1. **选择合适的激活函数**：要保持均值与方差不变，则激活函数需要满足其在零点处的切线斜率为1，截距为0。下图中除了sigmoid函数，其它几种函数都符合此条件：
	![[sigma.png|250]]
1. **梯度裁剪**：若梯度的模长大于某个值$\theta$，则通过投影将其模长拖回$\theta$。此方法可有效地预防梯度爆炸。
1. **选择合适的初始化**：设$h_{t-1}$独立于$W_t$，且令
	$$
	\left\{\begin{aligned}
	E(W_t)&=0\\
	D(W_t)&=\gamma_t
	\end{aligned}\right.
	$$
	其中$\gamma_t$为常数。可求出$h_t$的正向方差$D(h_t)$与反向梯度的方差$D\left(\frac{\partial \mathcal{L}}{\partial h_{t-1}}\right)$为
	$$
	\left\{\begin{aligned}
	D(h_t)=n_{t-1}\gamma_tD(h_{t-1})\\
	D\left(\frac{\partial \mathcal{L}}{\partial h_{t-1}}\right)=n_t\gamma_tD\left(\frac{\partial \mathcal{L}}{\partial h_t}\right)
	\end{aligned}\right.
	$$
	要保持方差不变，则需$n_{t-1}\gamma_t=n_t\gamma_t=1$，但此条件难以满足。此时可以运用Xavier初始化，即
	$$\gamma_t=\frac{2}{n_{t-1}+n_t}$$
	这样就使得$\gamma_t$能够尽量贴近于上述条件。所以初始权重可根据正态分布$\mathcal{N}\left(0,\sqrt{\frac{2}{n_{t-1}+n_t}}\right)$生成。
## 批量归一化
上述三种避免梯度爆炸与梯度消失的方法，其本质其实都是去约束每一层的分布，使得神经网络每一层的输出不至于跑得太偏。参考简单的机器学习任务中，为防止收敛速度过慢，输入一般都需要进行归一化处理。由此，我们可以在网络中的每一层都加上一步归一化，来约束其输出。

但若仅仅在每层添加归一化步骤，当神经网络很深时，由于损失的计算在最后，反向传播时顶部的层训练很快，底部的层则训练较慢，但底层一变化，顶层就得根据新的顶层重复学习，收敛速度同样不高。因此我们需要添加额外的参数，及时调整每层数据的分布。

> [!definition] 批量归一化 (Batch Normalization)
> 取一批数据在第$d$层的输出$x_{id}$均值$\mu_d$与方差$\sigma_d^2$，归一化后得出$\hat{x}_{id}$：
> $$\hat{x}_{id}=\frac{x_{id}-\mu_d}{\sigma_d}$$
> 添加额外的参数$\beta_d,\gamma_d$，得到最终的输出$y_{id}$：
> $$y_{id}=\gamma_{d}\hat{x}_{id}+\beta_d$$
> 模型训练过程中我们会不断地更新$\mu_{d}$与$\sigma_{d}^2$，但在测试阶段会固定这两个量。

此方法允许使用更高的学习率，从而加速模型收敛，但不会改变模型精度。

> [!note] 
> 有论文指出批量归一化就是通过在每个小批量里加入噪音来控制模型复杂度，因此不必与丢弃法混用。
## 残差网络
> [!definition] 残差网络 (Residual Net, ResNet)
> ResNet加入了一条与其它结构平行的快速通道，如果这些结构并未起到正向作用，那么输入将会由快速通道直接跳过这些结构。
> $$x_l=x_{l-1}+f(x_{l-1})$$
> ![[ResNet.png|275]]
> ResNet使卷积神经网络的深度能够达到上千层。

> [!caution] 
> *ResNet中两通道相加之前的那层不能是ReLU层。*因为ReLU层的输出一定是非负值，非负值不断叠加容易导致数值爆炸。

> [!note] 残差网络性能好的原因
> 我们可以将残差网络的公式$x_l=x_{l-1}+f(x_{l-1})$，表示成矩阵的形式，即$x_l=(I+W)x_{l-1}$。那么对于一个两层的残差网络，就有
> $$y=(I+W_1)(I+W_2)x$$
> 将其展开得到的是一个并联了四条路径的网络：
> $$y=Ix+W_1x+W_2x+W_1W_2x$$
> 由此类推，$L$层级联的残差网络，本质上其实是一个$2^{L}$路并联，且优化起来并不困难的网络。
## 全连接卷积神经网络

> [!definition] 全连接卷积神经网络 (Fully Convolutional Network, FCN)
> 其将CNN顶部的线性层改为了转置卷积层，最终得到与原图大小相等的输出，且实现对原图中每个像素的分类。
> ![[FCN.png|129]]
## SegNet
相较于FCN，SegNet的特点是十分的对称。
> [!definition] SegNet
> ![[segNet.jpg|625]]

此外，segNet还引入了池化索引。
> [!definition] 池化索引 (Pooling Index)
> 在池化层中记录下最大值的位置，然后在转置卷积层中将最大值填入相应的位置，其余位置填0，如下图所示：
> ![[segNet_unpool.png|625]]

SegNet的训练是逐层进行的，即首先训练外层，然后固定外层，添加内层进行训练。
![[segNet_train.png|600]]