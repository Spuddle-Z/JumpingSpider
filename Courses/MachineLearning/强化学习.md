---
tags:
  - Knowledge
aliases:
  - Reinforcement Learning
  - RL
---
## Alpha Go
### 基本概念
![[Pasted image 20240522184031.png|375]]
其由**策略网络(Policy Network)**与**价值网络(Value Network)**两个网络共同组成。
- 策略网络输出的是下一步棋该走什么地方的概率$P_{\sigma}(a|s)=softmax(h(s))$，其中$s$是当前棋盘状态，$a$是下一步行动，；
- 价值网络的输出是下在某处的潜在价值$v_{\theta}(s)$。

### 训练策略网络
1. **行为克隆(Behavior Cloning)**：以人类棋手的对弈数据为标签，进行监督训练
	$$\Delta\sigma\propto\frac{\partial }{\partial \sigma}\log P_{\sigma}(a^*|s)$$
1. **策略梯度(Policy Gradient)**：人类的对弈数据并不多，当通过行为克隆训练出一个较为粗糙的模型后，用其参数$\sigma$初始化新模型参数$\rho$，并用新模型与之对弈。如果输了，则$z=1$；如果赢了，则$z=-1$。
	$$\Delta\rho\propto\frac{\partial }{\partial \rho}\log P_{\rho}(a|s)\cdot z$$
	其本质上在提升
	$$E_{\rho}[z]=\int_{a,s}P(z=1|a_1,\cdots,a_{n},s_1,\cdots,s_{n+1})\cdot P_{\rho}(a_1,\cdots,a_{n},s_2,\cdots,s_{n+1}|s_1)$$
	其中
	$$P_{\rho}(a_1,...,a_n,s_2,...,s_{n+1}|s_1)=\prod_{i=1}^{n}P(s_{i+1}|a_i,s_i)P_{\rho}(a_i|s_i)$$
	$P(s_{i+1}|a_i,s_i)$是对手的下法；$P_{\rho}(a_i|s_i)$是我们的策略，是策略网络可以学习的分布。我们称这个架构为**动态模型(Dynamic Model)**。

> [!note] 
> 为避免每次下棋都采用同样的下法，对于行动的选择不是每次都选择获胜概率最大的下法，而是根据获胜概率随机采样，胜率大的下法被选中的概率就高。
### 训练价值网络
$$\Delta\theta\propto\frac{\partial }{\partial \theta}[z-v_{\theta}(s)]^2$$
其中$z$表示经过状态$s$的所有情况的平均胜率。但$z$极其不稳定，因为对于同一步棋来说，其实胜负并不确定。使用以下训练方法则只有在结果与平均胜率相差较大时才会有较大梯度，这样可以使得训练过程更加稳定。
$$\Delta\rho\propto\frac{\partial }{\partial \rho}\log P_{\rho}(a|s)[z-v_{\theta}(s)]$$
### 蒙特卡洛树搜索
> [!definition] 蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)
> ![[Pasted image 20240528103340.png|600]]
> 对于围棋中的每一个状态，我们可以如图画出一个十分庞大的搜索树。对于树中的每一种$s,a$，我们有：
> - $N(s,a)$为之前走过此状态的次数；
> - $Q(s,a)$为此状态的平均价值，即经过此状态的平均胜率。
> 我们希望去穷举所有的情况。此算法分为四个步骤：
> 1. **选择(Selection)**：选择一个节点来进行展开；
> 2. **扩展(Expasion)**：正常去下棋，直到最后一步；
> 3. **评估(Evaluation)**：判断是否获胜；
> 4. **反向传播(Backup)**：反向去更新路径上的$Q$。
> 
> 为兼顾胜率最大与探索更少的节点，即选择$Q$大$N$小的节点，我们每次都选择
> $$Q+\frac{C_{P_{\sigma}(a|s)}}{\sqrt{N(s,a)}}$$
> 这个指标最大的节点。

对于围棋来说，根本不可能穷举所有的这些步骤，因此我们的扩展步骤不需要下到最后一步得到$z$，我们只下有限的几步（假如是四步），然后直接把最后一步状态的$v_{\theta}(s_{4})$当作$z$来用（每个状态的$v_{\theta}$可以通过行为克隆来得到）。

我们令$R_{t}$为第$t$步之后的收益，即
$$R_t=\sum_{k=0}^{\infty}r_{t+k}$$
但由于越往后获得收益的不确定性越大，因此我们需要在上面加上一个$\gamma\in[0,1]$来递减更远步骤的影响
$$R_t=\sum_{k=0}^{\infty}r_{t+k}\gamma^{k}$$
我们令$X=(a_1,...,s_2,...)$代表已经下过的状态，再令$h(X)=R_0$。
$$E_{\rho}[h(X)]=\sum_{X}h(X)\cdot P(a_1,...,s_1,...|\rho)$$
我们通过最大化$R_0$来优化网络，同样使用梯度下降法：
$$
\begin{aligned}
\frac{\partial }{\partial \rho}E_{\rho}[R_0]&=\frac{\partial }{\partial \rho}E_{\rho}\left[ \sum_{t=1}^{\infty}r_t \right]\\
&=\frac{\partial }{\partial \rho}\sum_{a_1,...,s_1,...}\left[ \left( \sum_{t=1}^{\infty}r_t \right)\cdot P(a_1,...,s_1,...|\rho) \right]
\end{aligned}
$$
由于$\rho$只能控制选择某个走法的概率，而不能控制这个走法的收益，所以我们可以把偏导移入求和符号：
$$
\begin{aligned}
&\frac{\partial }{\partial \rho}\sum_{a_1,...,s_1,...}\left[ \left( \sum_{t=1}^{\infty}r_t \right)\cdot P(a_1,...,s_1,...|\rho) \right]\\
=&\sum_{a_1,...,s_1,...}\left[ \left( \sum_{t=1}^{\infty}r_t \right)\cdot \frac{\partial }{\partial \rho}P(a_1,...,s_1,...|\rho) \right]\\
=&\sum_{a_1,...,s_1,...}\left[ \left( \sum_{t=1}^{\infty}r_t \right)\cdot P(a_1,...,s_1,...|\rho)\frac{\partial }{\partial \rho}\log P(a_1,...,s_1,...|\rho) \right]\\
=&E_{\rho}\left[\left( \sum_{t=1}^{\infty}r_t \right)\cdot \frac{\partial }{\partial \rho}\left( \sum_{t=1}^{\infty}\log P_{\rho}(a_t|s_t)P(s_{t+1}|s_t,a_t) \right)\right]\\
=&E_{\rho}\left[\left( \sum_{t=1}^{\infty}r_t \right)\cdot \frac{\partial }{\partial \rho}\left( \sum_{t=1}^{\infty}\log \pi_{\rho}(a_t|s_t)\right)\right]\\
=&E_{\rho}\left[ \sum_{t=1}^{\infty}\left( \frac{\partial }{\partial \rho}\log\pi_{\rho}(a_t|s_t)\sum_{t'=1}^{\infty}r_{t'} \right) \right]
\end{aligned}
$$
其中$E_{\rho}$表示穷举每一盘棋，$\sum_{t=1}^{\infty}$表示穷举一盘棋中的每一步棋，$\sum_{t'=1}^{\infty}r_{t'}$其实就是$z$


0521-2-2404 #Missing 