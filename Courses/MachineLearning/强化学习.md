---
tags:
  - Knowledge
aliases:
  - Reinforcement Learning
  - RL
---
## Alpha Go
### 基本概念
![[Pasted image 20240522184031.png|375]]
其由**策略网络(Policy Network)**与**价值网络(Value Network)**两个网络共同组成。
- 策略网络输出的是下一步棋该走什么地方的概率$P_{\sigma}(a|s)=softmax(h(s))$，其中$s$是当前棋盘状态，$a$是下一步行动，；
- 价值网络的输出是下在某处的潜在价值$v_{\theta}(s)$。

### 训练策略网络
1. **行为克隆(Behavior Cloning)**：以人类棋手的对弈数据为标签，进行监督训练
	$$\Delta\sigma\propto\frac{\partial }{\partial \sigma}\log P_{\sigma}(a^*|s)$$
1. **策略梯度(Policy Gradient)**：人类的对弈数据并不多，当通过行为克隆训练出一个较为粗糙的模型后，用其参数$\sigma$初始化新模型参数$\rho$，并用新模型与之对弈。如果输了，则$z=1$；如果赢了，则$z=-1$。
	$$\Delta\rho\propto\frac{\partial }{\partial \rho}\log P_{\rho}(a|s)\cdot z$$
	其本质上在提升
	$$E_{\rho}[z]=\sum_{a,s}P(z=1|a_1,\cdots,a_{n},s_1,\cdots,s_{n+1})\cdot P(a_1,\cdots,a_{n},s_2,\cdots,s_{n+1}|s_1)$$
	其中
	$$P_{\rho}(a_1,...,a_n,s_2,...,s_{n+1}|s_1)=\prod_{i=1}^{n}P(s_{i+1}|a_i,s_i)P_{\rho}(a_i|s_i)$$
	$P(s_{i+1}|a_i,s_i)$是对手的下法；$P_{\rho}(a_i|s_i)$是我们的策略，是策略网络可以学习的分布。我们称这个架构为**动态模型(Dynamic Model)**。

> [!note] 
> 为避免每次下棋都采用同样的下法，对于行动的选择不是每次都选择获胜概率最大的下法，而是根据获胜概率随机采样，胜率大的下法被选中的概率就高。
### 训练价值网络
$$\Delta\theta\propto\frac{\partial }{\partial \theta}[z-v_{\theta}(s)]^2$$
其中$z$表示经过状态$s$的所有情况的平均胜率。但$z$极其不稳定，因为对于同一步棋来说，其实胜负并不确定。使用以下训练方法则只有在结果与平均胜率相差较大时才会有较大梯度，这样可以使得训练过程更加稳定。
$$\Delta\rho\propto\frac{\partial }{\partial \rho}\log P_{\rho}(a|s)[z-v_{\theta}(s)]$$
### 蒙特卡洛树搜索
> [!definition] 蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)
> ![[Pasted image 20240528103340.png|600]]
> 对于围棋中的每一个状态，我们可以如图画出一个十分庞大的搜索树。对于树中的每一种$s,a$，我们有：
> - $N(s,a)$为之前走过此状态的次数；
> - $Q(s,a)$为此状态的平均价值，即经过此状态的平均胜率。
> 我们希望去穷举所有的情况。此算法分为四个步骤：
> 1. **选择(Selection)**：选择一个节点来进行展开；
> 2. **扩展(Expasion)**：正常去下棋，直到最后一步；
> 3. **评估(Evaluation)**：判断是否获胜；
> 4. **反向传播(Backup)**：反向去更新路径上的$Q$。
> 
> 为兼顾胜率最大与探索更少的节点，即选择$Q$大$N$小的节点，我们每次都选择
> $$Q+\frac{C_{P_{\sigma}(a|s)}}{\sqrt{N(s,a)}}$$
> 这个指标最大的节点。


0521 #Missing 