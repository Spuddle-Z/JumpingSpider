---
tags:
  - Knowledge
aliases:
  - Machine Learning
---
# 概论
## 损失函数 Loss Function
### 基本概念
机器学习需要用给出的数据$\{(X_i,y_i)\}_{i=1,2,...,n}$去训练一个模型的参数$\beta$，使得将$X_i$输入模型后，模型的输出能尽量接近于$y_i$。这个训练的过程一般通过最小化损失函数$$\mathcal{L}(\beta)=\sum_{i=1}^nL(y_i,X_i^T\beta)$$来实现，其中$L(y_i,X_i^T\beta)$是其中第$i$组数据的损失。
### 常见损失函数
#### 线性回归的损失函数
##### Huber Loss
平方误差对于异常点十分敏感，因此引入Huber loss，即绝对值较低部分使用平方误差，$\delta$之外的部分则变为线性误差，其公式如下：
$$
L(y_i,X_i^T\beta)=
\left\{\begin{aligned}
&\frac{1}{2}(y_i,X_i^T\beta)^2 &,|y_i,X_i^T\beta|\leq\delta \\
&\delta|y_i,X_i^T\beta|-\frac{\delta^2}{2} &,\text{otherwise}
\end{aligned}\right.
$$
![[Pasted image 20240305110220.png|400]]
#### 分类模型的损失函数
##### 逻辑回归损失函数
对于逻辑回归，我们需要最大化其似然函数$$\prod_{i=1}^{n}P(y_{i}|X_{i},\beta)$$，此函数代表模型参数为$\beta$，输入为$X$时，输出为$y$的概率。
1. 若$y_{i}\in\left\{0,1\right\}$，我们称其为0/1响应，此时
	$$
	\begin{aligned}
	P(y_{i}|X_{i},\beta)&=P(y_{i}=1|X_{i},\beta)^{y_{i}}P(y_{i}=0|X_{i},\beta)^{1-y_{i}}\\
	&=\left[ \frac{\exp(X_{i}^{T}\beta)}{1+\exp(X_{i}^{T}\beta)} \right]^{y_{i}}\left[ \frac{1}{1+\exp(X_{i}^{T}\beta)} \right]^{1-y_{i}}\\
	&=\frac{\exp(y_{i}X_{i}^{T}\beta)}{1+\exp(X_{i}^{T}\beta)}
    \end{aligned}
    $$
    而为了简便计算，我们将最终的损失函数定为负的对数似然函数$$L(y_i,X_i^T\beta)=-\log P(y_{i}|X_{i},\beta)=-[y_i,X_i^T\beta-\log(1+\exp(X_{i}^{T}\beta))]$$
1. 若$y_{i}\in\{+1,-1\}$，我们称为$\pm$响应，此时
	$$
	\left\{\begin{aligned}
    P(y_{i}=+1)&=\frac{1}{1+\exp(-X_{i}^{T}\beta)}\\
    P(y_{i}=-1)&=\frac{1}{1+\exp(X_{i}^{T}\beta)}
    \end{aligned}\right.
	$$
	其损失函数同样取负对数似然函数$$L(y_i,X_i^T\beta)=\log[1+\exp(-y_iX_i^T\beta)]$$
综上，我们可以得到logistic loss为$$\log(1+\exp(-y_iX_i^{T}\beta))$$
##### 其它分类模型损失函数
除了logistic loss，我们还有以下几种损失函数：![[Pasted image 20240311210123.png|450]]
- Hinge loss：此函数计算较logistic loss更为简便，一般用于支持向量机。$$\max(0,1-y_iX_i^T\beta)$$
- Exponential loss：此函数一般用于AdaBoost。$$\exp(-y_iX_i^T\beta)$$
- Zero-one loss：此函数无法求导，一般用于计算被错误分类的样本个数。$$\mathbb{1}(y_iX_i^T\beta<0)$$

由图可以看出，hinge loss与exponential loss其实都是对logistic loss的近似。

图中横轴为$m_{i}=y_{i}X_{i}^{T}\beta$，也即这四种函数均是基于$y_{i}X_{i}^{T}\beta$的，我们将此值$m_{i}$称为样本$(y_{i},X_{i})$的**margin**。Margin为负时，模型将受到惩罚，越负惩罚越大；当margin为比较小的正值时，依然会受到一定的惩罚，因此我们其实是希望margin能够尽可能地大，以此巩固已经正确了的分类结果。

---
## 三种模型
**判定模型(Discriminative Model)**的目标是最大化$P_\theta(y|X)$，也即在输入为$X$的情况下，令参数为$\theta$的模型输出$y$的概率最大。训练判定模型的本质是根据已知的$X,y$，找到$P_{\theta}(y|X)$的[[概率论#^tyfz0r|最大似然估计]]，也即是一个最小化$P_{\theta}(y|X)$与$P_{Data}(y|X)$之间的[[信息论#^ejkg5c|KL散度]]的过程。其推导过程如下：
$$
\begin{aligned}
\max_{\theta}\frac{1}{n}\sum_{i=1}^{n}\log P_{\theta}(y_i|X_i)
&\equiv\max_{\theta}E_{P_{Data}}(\log P_{\theta}(y|X))\\
&\equiv\min_{\theta}-E_{P_{Data}}(\log P_{\theta}(y|X))\\
&\equiv\min_{\theta}E_{P_{Data}}(\log P_{Data}(y|X))-E_{P_{Data}}(\log P_{\theta}(y|X))\\
\end{aligned}
$$

**描述性模型(Description Model)**

**生成式模型(Generative Model)**：
## 布朗运动
- **布朗运动(Brownian Motion)**可以定义为$$X_{t+\Delta t}=X_{t}+\sigma\varepsilon_{t}\sqrt{\Delta t}$$

寻找损失函数最小值的过程可以看作布朗运动的过程，如下式：$$\theta_{new}=\theta_{old}-\eta\frac{\partial L}{\partial \theta}+\lambda\varepsilon$$
## SVM
为与其它教材统一，我们规定如下符号：
- $b=\beta_1$
- $w=[\beta_2,\beta_3,\cdots,\beta_{n}]^T$
- margin $\gamma_i=y_i(w^Tx_i+b)$

我们需要让margin尽可能大，但如果直接最大化$\gamma_i$，由于$w,b$都是不确定的参数，$w,b$可能会水涨船高变得无限大，因此需要除以其模长控制其大小，即调整$\gamma_i$为$$\gamma_i=y_i\frac{w^Tx_i+b}{||w||}$$
但实际上，我们需要最大化的只是所有数据中最小的那个margin，设其为$\tau$，则有
$$\max_{\tau,w,b}\frac{\tau}{||w||}\quad s.t.\forall i,y_i(w^Tx_i+b)\geq\tau$$
将$\tau$固定，则最大化$\frac{\tau}{||w||}$等同于最小化$||w||$，再将其平方使其光滑，前面加上系数方便求导，则上式变为
$$\min_{w,b}\frac{1}{2}||w||^2\quad s.t.\forall i,y_i(w^Tx_i+b)\geq1$$
此优化问题的[[凸优化#拉格朗日乘子法|拉格朗日函数]]为
$$L(w,b,\lambda)=\frac{1}{2}||w||^2-\sum_{i=1}^{n}\lambda_i\left[y_i(w^TX_i+b)-1\right]$$
根据[[凸优化#KKT条件|KKT条件]]，使
$$\frac{\partial L}{\partial w}=\frac{\partial L}{\partial b}=0$$
推得
$$
\left\{\begin{aligned}
\sum_{i=1}^{n}\lambda_iy_iX_i&=w\\
\sum_{i=1}^{n}\lambda_iy_i&=0
\end{aligned}\right.
$$
将其代入$L(w,b,\lambda)$，得到
$$L(w,b,\lambda)=\sum_{i=1}^{n}\lambda_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}y_iy_j\lambda_i\lambda_jX_i^TX_j$$

#broken
### 含outliers
#broken 
$$
\begin{aligned}
\min_{\xi,w,b}&\quad\frac{1}{2}||w||^2+C\sum_{i=1}^{n}\xi_i\\
s.t.&\quad y_i(w^Tx_i+b)\geq1-\xi_i\\
&\quad\xi_i\geq0
\end{aligned}
$$
此时其约束条件可以总结为
$$s.t.\quad \xi_i\geq\max(0,1-y_i(w^Tx_i+b))$$

---

因此，我们有
$$
\hat{y}=sign(w^Tx+b)=
\left\{\begin{aligned}
+1,&w^Tx+b\geq0\\
-1,&w^Tx+b<0
\end{aligned}\right.
$$






由于只有支持向量的$\alpha_i>0$，因此$w$的方向只由支持向量决定。