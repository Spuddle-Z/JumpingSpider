---
tags:
  - Knowledge
aliases:
  - Machine Learning
---
## 损失函数 Loss Function
### 基本概念
机器学习需要用给出的数据$\{(X_i,y_i)\}_{i=1,2,...,n}$去训练一个模型的参数$\beta$，使得将$X_i$输入模型后，模型的输出能尽量接近于$y_i$。这个训练的过程一般通过最小化损失函数$$\mathcal{L}(\beta)=\sum_{i=1}^nL(y_i,X_i^T\beta)$$来实现，其中$L(y_i,X_i^T\beta)$是其中第$i$组数据的损失。
### 常见损失函数
#### 线性回归的损失函数
##### Huber Loss
平方误差对于异常点十分敏感，因此引入Huber loss，即绝对值较低部分使用平方误差，$\delta$之外的部分则变为线性误差，其公式如下：
$$
L(y_i,X_i^T\beta)=
\left\{\begin{aligned}
&\frac{1}{2}(y_i,X_i^T\beta)^2 &,|y_i,X_i^T\beta|\leq\delta \\
&\delta|y_i,X_i^T\beta|-\frac{\delta^2}{2} &,\text{otherwise}
\end{aligned}\right.
$$
![[Pasted image 20240305110220.png|400]]
#### 分类模型的损失函数
##### 逻辑回归损失函数
对于逻辑回归，我们需要最大化其似然函数$$\prod_{i=1}^{n}P(y_{i}|X_{i},\beta)$$，此函数代表模型参数为$\beta$，输入为$X$时，输出为$y$的概率。
1. 若$y_{i}\in\left\{0,1\right\}$，我们称其为0/1响应，此时
	$$
	\begin{aligned}
	P(y_{i}|X_{i},\beta)&=P(y_{i}=1|X_{i},\beta)^{y_{i}}P(y_{i}=0|X_{i},\beta)^{1-y_{i}}\\
	&=\left[ \frac{\exp(X_{i}^{T}\beta)}{1+\exp(X_{i}^{T}\beta)} \right]^{y_{i}}\left[ \frac{1}{1+\exp(X_{i}^{T}\beta)} \right]^{1-y_{i}}\\
	&=\frac{\exp(y_{i}X_{i}^{T}\beta)}{1+\exp(X_{i}^{T}\beta)}
    \end{aligned}
    $$
    而为了简便计算，我们将最终的损失函数定为负的对数似然函数$$L(y_i,X_i^T\beta)=-\log P(y_{i}|X_{i},\beta)=-[y_i,X_i^T\beta-\log(1+\exp(X_{i}^{T}\beta))]$$
1. 若$y_{i}\in\{+1,-1\}$，我们称为$\pm$响应，此时
	$$
	\left\{\begin{aligned}
    P(y_{i}=+1)&=\frac{1}{1+\exp(-X_{i}^{T}\beta)}\\
    P(y_{i}=-1)&=\frac{1}{1+\exp(X_{i}^{T}\beta)}
    \end{aligned}\right.
	$$
	其损失函数同样取负对数似然函数$$L(y_i,X_i^T\beta)=\log[1+\exp(-y_iX_i^T\beta)]$$
综上，我们可以得到logistic loss为$$\log(1+\exp(-y_iX_i^{T}\beta))$$
##### 其它分类模型损失函数
除了logistic loss，我们还有以下几种损失函数：![[Pasted image 20240311210123.png|450]]
> [!definition] Hinge Loss
> 此函数计算较logistic loss更为简便，一般用于支持向量机。
> $$\max(0,1-y_iX_i^T\beta)$$ ^5tx2cc

> [!definition] Exponential Loss
> 此函数一般用于AdaBoost。
> $$\exp(-y_iX_i^T\beta)$$

> [!definition] Zero-one Loss
> 此函数无法求导，一般用于计算被错误分类的样本个数。
> $$\mathbb{1}(y_iX_i^T\beta<0)$$

由图可以看出，hinge loss与exponential loss其实都是对logistic loss的近似。

图中横轴为$m_{i}=y_{i}X_{i}^{T}\beta$，也即这四种函数均是基于$y_{i}X_{i}^{T}\beta$的，我们将此值$m_{i}$称为样本$(y_{i},X_{i})$的**margin**。Margin为负时，模型将受到惩罚，越负惩罚越大；当margin为比较小的正值时，依然会受到一定的惩罚，因此我们其实是希望margin能够尽可能地大，以此巩固已经正确了的分类结果。

---
## 三种模型
**判定模型(Discriminative Model)**：的目标是最大化$P_\theta(y|X)$，也即在输入为$X$的情况下，令参数为$\theta$的模型输出$y$的概率最大。训练判定模型的本质是根据已知的$X,y$，找到$P_{\theta}(y|X)$的[[概率论#^tyfz0r|最大似然估计]]，也即是一个最小化$P_{\theta}(y|X)$与$P_{Data}(y|X)$之间的[[信息论#^ejkg5c|KL散度]]的过程。其推导过程如下：
$$
\begin{aligned}
\max_{\theta}\frac{1}{n}\sum_{i=1}^{n}\log P_{\theta}(y_i|X_i)
&\equiv\max_{\theta}E_{P_{Data}}(\log P_{\theta}(y|X))\\
&\equiv\min_{\theta}-E_{P_{Data}}(\log P_{\theta}(y|X))\\
&\equiv\min_{\theta}E_{P_{Data}}(\log P_{Data}(y|X))-E_{P_{Data}}(\log P_{\theta}(y|X))\\
\end{aligned}
$$

**描述性模型(Description Model)**：其目标是描述一批数据的分布，即最大化$P_{\theta}(X)$。

**生成式模型(Generative Model)**：
## 布朗运动
- **布朗运动(Brownian Motion)**可以定义为$$X_{t+\Delta t}=X_{t}+\sigma\varepsilon_{t}\sqrt{\Delta t}$$

寻找损失函数最小值的过程可以看作布朗运动的过程，如下式：$$\theta_{new}=\theta_{old}-\eta\frac{\partial L}{\partial \theta}+\lambda\varepsilon$$
## 核与正则化
### SVM
为与其它教材统一，我们规定如下符号：
- $b=\beta_0$
- $w=[\beta_1,\beta_2,\cdots,\beta_{n}]^T$

由此我们可以看到$X_i^T\beta$变为了$w^Tx_i+b$。

SVM相关内容具体参考：
- [[SVM#基本概念|基本概念]]
- [[SVM#原理|具体原理]]
- [[SVM#核方法与核技巧|核方法与核技巧]]
- [[SVM#软边界SVM|软边界SVM]]
### 岭回归
在多元线性回归中，其数据的变量数可能会超过其样本数，造成$X^TX$不满秩，解出无限多个$\beta$。此时引入正则化项，来惩罚过多的特征，即**岭回归(Ridge Regression)**。

岭回归相关内容具体参考：
- [[岭回归#岭回归|岭回归]]
- [[岭回归#核回归|核回归]]
- [[岭回归#贝叶斯回归|贝叶斯回归]]
- [[岭回归#高斯过程|高斯过程]]

> [!note] SVM与岭回归关系
> 对比二者的目标函数
> - SVM：
> 	$$\frac{1}{2}||w||^2+C\sum_{i=1}^{n}\max(0,1-y_i(w^Tx_i+b))$$
> - 岭回归：
> 	$$||Y-X\beta||^2+\lambda||\beta||^2$$
> 
> 可以发现$||w||^2$即对应于$||\beta||^2$，可见SVM也同样用着岭回归的思想。

> [!note] 神经网络与岭回归
> 神经网络中会有一步weight decay，即令梯度减去一个$\lambda\Theta$，这其实是$\frac{\lambda}{2}||\Theta||^2$的导数。
### 样条回归
> [!definition] 样条回归 (Spline Regression)
> ![[Pasted image 20240419113527.png|250]]
> 通过分段线性模型来进行拟合。其损失函数如下
> $$\sum_{i=1}^{n}\left\lVert y_i-\alpha_0-\sum_{j=1}^{p}\alpha_j\max(0,x_i-k_j)\right\rVert^2+\lambda\sum_{j=1}^{p}\alpha_j^2$$

将其改写为矩阵的表达形式
$$l(\alpha)=\left\lVert Y-Z\alpha\right\rVert^2+\lambda\left\lVert D\alpha\right\rVert^2$$
其中
$$
\left\{\begin{aligned}
Z&=
\begin{bmatrix}
1&\max(0,x_1-k_1)&\cdots&\max(0,x_1-k_p)\\
1&\max(0,x_2-k_1)&\cdots&\max(0,x_2-k_p)\\
\vdots&\vdots&\ddots&\vdots\\
1&\max(0,x_n-k_1)&\cdots&\max(0,x_n-k_p)
\end{bmatrix}\\
\alpha&=[\alpha_0,\alpha_1,...,\alpha_p]^T\\
D&=\text{diag}(0,1,...,1)
\end{aligned}\right.
$$
与岭回归和核回归同理可得
$$\hat{\alpha}_{\lambda}=(Z^TZ\lambda D)^{-1}Z^TY$$
> [!note] 
> 神经网络的本质也是一个样条回归。为神经网络引入非线性特征的是ReLU函数，其本质就是一个$\max(0,x+1)$，因此神经网络所拟合出的模型其实是由一个个小的线性部分拼接而成的。
### Lasso回归
#### 基本概念
> [!definition] Lasso回归 (Lasso Regression)
> 其形式与岭回归相似，只是正则项的系数降为1。其优化的目标函数为
> $$\hat{\beta}_{\lambda}=\mathop{\arg\!\min}\limits_{\beta}\ \left[ \frac{1}{2}||Y-X\beta||^2+\lambda||\beta|| \right]$$

Lasso回归有一解析解：
$$
\hat{\beta}_{\lambda}=
\left\{\begin{aligned}
\frac{X^TY-\lambda}{||X||^2}\quad&,X^TY>\lambda\\
\frac{X^TY+\lambda}{||X||^2}\quad&,X^TY<-\lambda\\
0\quad&,-\lambda\leq X^TY\leq\lambda
\end{aligned}\right.
$$
令$\gamma=\frac{X^TY}{||X||^2}$，则$\hat{\beta}_{\lambda}$图像可以表示为
![[Pasted image 20240429222738.png|300]]
加入此正则化项之后，会将所有$\beta$的绝对值下降$\frac{\lambda}{||X||^2}$，一些较小的$\beta$会直接降至0，其效果即是直接忽略一些无关的特征。
> [!note] Lasso回归与岭回归正则化项的作用区别
> - 岭回归的正则化项会令各个特征权重的分布更均匀，避免出现一些绝对主导的特征；
> - 而Lasso回归则会直接将一些不重要特征的权重减少到零，达到稀疏特征的目的。
#### 原形式与对偶形式
> [!definition] Lasso回归的原形式 (Primal Form) 与对偶形式 (Dual Form)
> - 原形式：
> 	$$
> 	\begin{aligned}
> 	\min_{\beta}\ &\frac{||Y-X\beta||^2}{2}\\
> 	\text{s.t.}\quad&||\beta||\leq t
> 	\end{aligned}
> 	$$
> - 对偶形式：
> 	$$\min_{\beta}\frac{||Y-X\beta||^2}{2}+\lambda||\beta||$$

两种形式是*完全等价*的，即原形式的最优解也是对偶形式的最优解。
> [!proof] 
> 可以由反证法证明。我们令
> $$
> \left\{\begin{aligned}
> \hat{\beta}_{\lambda}&=\mathop{\arg\!\min}\limits_{\beta}\ \frac{1}{2}||Y-X\beta||^2+\lambda||\beta||\\
> t&=||\hat{\beta}\lambda||\\
> \hat{\beta}&=\mathop{\arg\!\min}\limits_{\beta}\ \frac{1}{2}||Y-X\beta||^2\qquad\text{s.t.}\quad||\hat{\beta}||\leq t
> \end{aligned}\right.
> $$
> 且$\hat{\beta}_{\lambda}\neq\hat{\beta}$。那么由以上条件易得
> $$
> \left\{\begin{aligned}
> \frac{1}{2}||Y-X\hat{\beta}||^2&<\frac{1}{2}||Y-X\hat{\beta}_{\lambda}||^2\\
> ||\hat{\beta}||&<||\hat{\beta}_{\lambda}||
> \end{aligned}\right.
> $$
> 以上两式线性组合，得到
> $$\frac{1}{2}||Y-X\hat{\beta}||^2+\lambda||\hat{\beta}||<\frac{1}{2}||Y-X\hat{\beta}_{\lambda}||^2+\lambda||\hat{\beta}_{\lambda}||$$
> 矛盾，QED.
#### 求解Lasso回归
求解其原形式或对偶形式均可，但若使用梯度下降法求解，由于$||\beta||$的梯度不连续，其收敛到足够小时可能会发生跳变，使得其loss曲线一直在波动，无法收敛。

工学上的解决方法如下：
![[Pasted image 20240430111527.png|650]]
- 对于内层循环：不直接用$X\beta$拟合$Y$，而是一维一维地去拟合，即令$X_j\beta_j$去拟合$R_j=Y-\sum_{k\neq j}X_k\beta_k$。
- 对于外层循环：令$\lambda$由大到小变化。$\lambda$较大时，会使得较多的$\beta_j$为零；随着$\lambda$变小，$\beta$也慢慢精细。
### 解空间
对于任何以下形式的损失函数
$$\sum_{i=1}^{n}L(y_i,x_i^T\beta)+\lambda||\beta||^2$$
其最优解都为$x_i$的线性组合，即
$$\hat{\beta}=\sum_{i=1}^{n}\alpha_ix_i$$
> [!proof] 
> 假设其最优解为
> $$\tilde{\beta}=\sum_{j=1}^{n}\alpha_jx_j+\sum_{k=1}^{K}\kappa_kx_k'$$
> 其中$x\perp x'$，则有
> $$
> \begin{aligned}
> x_i^T\beta&=\sum_{j=1}^{n}\alpha_jx_i^Tx_j+\sum_{k=1}^{K}\kappa_kx_i^Tx_k'\\
> &=x_i^T\sum_{j=1}^{n}\alpha_jx_j\\
> &=x_i^T\hat{\beta}
> \end{aligned}
> $$
> 而$||\hat{\beta}||^2\leq||\tilde{\beta}||^2$，因此
> $$\sum_{i=1}^{n}L(y_i,x_i^T\hat{\beta})+\lambda||\tilde{\beta}||^2\leq\sum_{i=1}^{n}L(y_i,x_i^T\tilde{\beta})+\lambda||\tilde{\beta}||^2$$
> 矛盾，QED.

同理，使用核方法后也有$\hat{\beta}=\sum_{i=1}^{n}\alpha_i\phi(x_i)$。
## 深度学习
### 基本神经网络
- [[Courses/MachineLearning/神经网络#基本概念|基本概念]]
- [[Courses/MachineLearning/神经网络#反向传播|反向传播]]
- [[Courses/MachineLearning/神经网络#梯度下降|梯度下降]]
### 卷积神经网络
- [[CNN#背景|背景]]
- [[CNN#基础CNN|基础CNN]]
- [[CNN#AlexNet、VGG、NiN与Inception|AlexNet、VGG、NiN与Inception]]
- [[CNN#梯度爆炸与梯度消失|梯度爆炸与梯度消失]]
- [[CNN#批量归一化|批量归一化]]
- [[CNN#残差网络|残差网络]]
### 循环神经网络
- [[RNN#背景|背景]]
- [[RNN#基础RNN|基础RNN]]
- [[RNN#LSTM|LSTM]]
- [[RNN#GRU|GRU]]
### GAN与VAE
- [[GAN#转置卷积|转置卷积]]
- [[GAN#Auto-Encoder|Auto-Encoder]]
- [[GAN#GAN|GAN]]
- [[VAE#AE到VAE|AE到VAE]]
- [[VAE#损失函数意义|VAE损失函数意义]]
## 降维与可视化
- 降维方法：[[PCA]]
- 可视化方法：
	- [[高维数据可视化方法#t-SNE|t-SNE]]
	- [[高维数据可视化方法#LLE|LLE]]
