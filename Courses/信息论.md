---
tags:
  - Knowledge
aliases:
  - Information Theory
---
## 基本概念
事件的概率越小，不确定性越大，其一旦发生所带来的信息量越大。
> [!definition] 自信息 (Self-information)
> 对于事件$x$，其发生的概率为$P(x)$，则其自信息定义为
> $$I(x)=-\log p$$
> 此处底数统一取2。

此时自信息还可以表示*最优编码长度*，如下表：![[Pasted image 20240316205049.png|450]]因此，自信息的单位为*比特(Bit)*。
> [!definition] 熵 (Entropy)
> 也称平均自信息，是用来量化一个随机变量不确定性的概念。对于随机变量$X=\{x_1,x_2,\cdots,x_{n}\}$，其中$x_i$发生的概率为$P(x_i)$，熵的公式如下：$$H(X)=-\sum_{i=1}^{n}P(x_{i})\log(P(x_{i}))$$ ^6us1wf

由自信息的定义可知，熵还可以表示最优平均编码长度，但由于自信息值可能取不到整数，因此真正编码时，只能用次优编码长度$-\sum_{i=1}^{n}P(x_i)\log(Q(x_i))$，且有$$H(X)=-\sum_{i=1}^{n}P(x_i)\log(P(x_i))\leq-\sum_{i=1}^{n}P(x_i)\log(Q(x_i))$$
将$P,Q$看作两个分布，即$\int P(x)\ dx=\int Q(x)\ dx=1$，可以引出KL距离。
> [!definition] KL散度 (Kullback-Leibler Divergence)
> KL距离表示两个分布的相似度。对于两个分布$P(x),Q(x)$，其KL距离定义为$$KL(P(x)||Q(x))=\sum_{x}P(x)\log\frac{P(x)}{Q(x)}$$^ejkg5c

KL距离有以下特点：
1. $KL(P|Q)\geq0$
2. $KL(P|Q)=0$时，$P=Q$
3. $KL(P|Q)\neq KL(Q|P)$
4. $KL(P(x,y)|Q(x,y))=KL(P(x)|Q(x))+KL(P(y|x)|Q(y|x))$

对于联合概率分布，则有如下推导：
$$
\begin{aligned}
KL(P(y|x)|Q(y|x))&=\sum_{(x,y)\sim P(x,y)}P(x,y)\log\frac{P(y|x)}{Q(y|x)}\\
&=\sum_{x}P(x)\sum_{y}P(y|x)\log\frac{P(y|x)}{Q(y|x)}
\end{aligned}
$$
为解决KL散度不对称性所带来的问题，我们引入对称的JS散度。
> [!definition] JS散度 (Jenson-Shannon Divergence, JSD)
> 对于两个分布$P(x),Q(x)$，令$M(x)=\frac{1}{2}(P(x)+Q(x))$，则
> $$JSD(P|Q)=\frac{1}{2}KL(P|M)+\frac{1}{2}KL(Q|M)$$ ^u7htjl