---
tags:
  - Knowledge
aliases:
  - Probability Theory
---
## 随机事件和概率
### 基本概念
> [!definition] 样本空间
> 一次试验的所有可能结果。

> [!definition] 古典概型
> 1. **有限性**：样本空间中只有有限个基本事件；
> 2. **等可能性**：每个基本事件的发生概率相同。
> 
> 符合以上两个特征的试验，称为古典概型。

> [!example] 分组问题
> 将$n$个不同元素分成$m$个不同的组，各组分别有$k_1,...,k_m$个元素，$\sum_{i=1}^{n}k_i=n$，不同的分法有$C_n^{k_1}C_{n-k_1}^{k_2}\cdots C_{k_{m}^{k_{m}}}$种。

> [!definition] 加法定理
> 对任意事件$A,B$，有$P(A\cup B)=P(A)+P(B)-P(AB)$。

加法定理的推广：
$$P\left(\bigcup_{i=1}^{n}A_i\right)=\sum_{i=1}^{n}P(A_i)-\sum_{1\leq i<j\leq n}P(A_iA_j)+\sum_{1\leq i<j<k\leq n}P(A_iA_jA_k)+\cdots+(-1)^{n-1}P(A_1A_2\cdots A_n)$$

### 条件概率
在$A$发生的条件下，发生事件$B$，记作事件$B|A$。则易证
$$P(B|A)=\frac{P(AB)}{P(A)}$$

## 随机变量及其分布
### 离散型随机变量极其分布
> [!definition] Bernoulli概型
> 符合下面条件的试验所对应的概型：
> - 每次试验结果只有两个；
> - 每次试验结果相互独立。

> [!definition] 二项分布
> 令$X$为$n$次试验中事件$A$的发生次数，单次试验中$A$的发生概率为$p$，则$X$服从参数为$n,p$的二项分布，即$X\sim B(n,p)$，且有
> $$P(X=k)=C_n^{k}p^{k}(1-p)^{n-k}$$

> [!definition] Poisson分布
> 令随机变量$X$的样本空间为$\mathbb{N}$，则下面分布律称为服从参数$\lambda$的Poisson分布，记为$X\sim P(\lambda)$：
> $$P(X=k)=e^{-\lambda}\frac{\lambda^{k}}{k!}$$

> [!theorem] Poisson定理
> 在二项分布中，令$n$变大的同时$p_n$随之变小，且满足$\lim_{n\to\infty}np_n=\lambda>0$，则有
> $$\lim_{n\to\infty}C_n^{k}p_n^{k}(1-p_n)^{n-k}=e^{-\lambda}\frac{\lambda^{k}}{k!}$$
> 即二项分布的极限分布为Poisson分布。

### 连续型随机变量及其分布
> [!definition] 均匀分布
> 若$X$的概率密度函数为
> $$
> f(x)=
> \left\{\begin{aligned}
> &\frac{1}{b-a}&,a<x<b\\
> &0&,\text{otherwise}
> \end{aligned}\right.
> $$
> 则称$X$服从$(a,b)$上的均匀分布，记作$X\sim U(a,b)$。

> [!definition] 指数分布
> 若$X$的概率密度函数为
> $$
> f(x)=
> \left\{\begin{aligned}
> &\lambda e^{-\lambda x}&,x>0\\
> &0&,x\leq0
> \end{aligned}\right.
> $$
> 则称$X$服从$\lambda$的指数分布，记作$X\sim E(\lambda)$。

> [!definition] 正态分布
> 若$X$的概率密度函数为
> $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\quad-\infty<x<+\infty$$
> 则称$X$服从参数为$\mu,\sigma$的正态分布，记作$X\sim N(\mu,\sigma^2)$。其中$\left\{\begin{aligned}\mu=0\\\sigma=1\end{aligned}\right.$的正态分布为**标准正态分布**。

标准正态分布的分布函数记为$\Phi(x)$。

一般正态分布概率可以通过转化成标准正态分布计算。设$X\sim N(\mu,\sigma^2)$的分布函数为$F(x)$，则有
$$F(x)=\Phi\left( \frac{x-\mu}{\sigma} \right)$$

### 随机变量函数的分布
对于*离散型随机变量*$Y=g(X)$，一般是先求出$Y$的所有可能取值，再确定其每个取值下的概率。

对于*连续型随机变量*$Y=g(X)$，一般先求$Y$的分布函数$F_{Y}(y)$，再对其求导得到$Y$的概率密度函数$f_{Y}(y)=\frac{d}{dy}F_{Y}(y)$。

### 基本概念
> [!definition] 期望 (Expectation)
> 度量一个随机变量取值的集中位置或平均水平的最基本的数字特征。

期望的运算性质：
1. $E(cX)=cE(X)$
2. $E(X\pm Y)=E(X)\pm E(Y)$
3. 若$X,Y$相互独立，则有$E(XY)=E(X)E(Y)$

> [!definition] 方差 (Variance)
> 表示随机变量取值的分散性的一个数字特征，一般用$\sigma^2$或$D(X)$表示，可以由期望按下式算得
> $$D(X)=E[(X-E(X))^2]=E(X^2)-E(X)^2$$

方差的运算性质：
1. $D(c)=0$
2. $D(cX)=c^2D(X)$
3. $D(X+c)=D(X)$
4. 若$X,Y$相互独立，则有$D(X\pm Y)=D(X)+D(Y)$

> [!definition] 协方差 (Covariance)
> 用来衡量两个随机变量的变化趋势是否一致。对于两个随机变量$X,Y$，其协方差为$$\sigma(X,Y)=E\left[(X-E(X))\cdot(Y-E(Y))\right]=E(XY)-E(X)E(Y)$$

从数值上来说，协方差越大，说明两个变量的同向趋势越大；协方差越小，说明两个变量的反向趋势越大；协方差越趋近于零，两个变量越不相关。

> [!definition] 协方差矩阵
> 对于$d$个随机变量，我们可以求出一个协方差矩阵：
> $$
> \Sigma=
> \begin{bmatrix}
> \sigma(X_1,X_1) & \cdots & \sigma(X_1,X_d) \\
> \vdots & \ddots & \vdots \\
> \sigma(X_d,X_1) & \cdots & \sigma(X_d,X_d)
> \end{bmatrix}
> $$
> ^yvq7oj

### 运算规律
若$a,b$为两个*相互独立*的随机变量，则有
$$
\left\{\begin{aligned}
a\sim N(\mu_1,\sigma_1^2)\\
b\sim N(\mu_2,\sigma_2^2)
\end{aligned}\right.
\implies a+b\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)
$$
对于两个向量$\alpha,\beta$，则有
$$
\left\{\begin{aligned}
\alpha\sim N(\mu_{\alpha},\sigma_{\alpha}^2)\\
\beta\sim N(\mu_{\beta},\sigma_{\beta}^2)
\end{aligned}\right.
\implies
\begin{bmatrix}
\alpha \\ \beta
\end{bmatrix}
\sim N\left(
\begin{bmatrix}
\mu_{\alpha}\\\mu_{\beta}
\end{bmatrix},
\begin{bmatrix}
\Sigma_{\alpha\alpha}&\Sigma_{\alpha\beta}\\
\Sigma_{\beta\alpha}&\Sigma_{\beta\beta}
\end{bmatrix}
\right)
$$
且有
$$P(\beta|\alpha)\sim N(\mu_{\beta}+\Sigma_{\beta\alpha}\Sigma_{\alpha\alpha}^{-1}(\alpha-\mu_{\alpha}),\Sigma_{\beta\beta}-\Sigma_{\beta\alpha}\Sigma_{\alpha\alpha}^{-1}\Sigma_{\alpha\beta})$$

## 参数估计
> [!definition] 似然函数 (Likelihood Function)
> 对于函数$P(x|\theta)$，其中$x$表示实际数据，$\theta$表示模型参数，则若$x$固定，$P(x|\theta)$随$\theta$变化，那么这个函数叫做似然函数。

> [!note] 似然函数与概率函数
> 若$\theta$固定，$P(x|\theta)$随着$x$变化，则此函数是一个固定模型的概率函数。

> [!definition] 最大似然估计 (Maximum Likelihood Estimation, MLE)
> 当我们有一组数据$x_1,x_2,\cdots,x_{n}$时，我们需要求出那个最可能得出这组数据的模型参数$\theta^*$，也即求$$\mathop{\arg\max}\limits_{\theta}\prod_{i=1}^{n}P(x_i|\theta)\equiv\mathop{\arg\max}\limits_{\theta}\sum_{i=1}^{n}\log P(x_i|\theta)$$ ^tyfz0r
## Logit
> [!definition] Odds
> 是用来表示概率大小的量，不过概率的取值范围为$[0,1]$，而odds的取值范围为$[0,+\infty)$，其公式为
> $$Odds(P)=\frac{P}{1-P}$$

> [!definition] Logit函数
> 在odds的基础上取对数，即可得到Logit
> $$Logit(P)=\log Odds(P)=\log\frac{P}{1-P}$$
> 其图像如下：
> ![[Pasted image 20240317154521.png|400]]

> [!definition] Sigmoid函数
> 是Logit的反函数
> $$\sigma(x)=\frac{1}{1+e^{-x}}$$
> ![[Pasted image 20240317155319.png|400]]
## 数据的基本统计描述
### 中心趋势
对于平均数(Mean)、中位数(Median)与众数(Mode)，在数据分布为*单峰*时有此经验公式：
$$Mean-Mode\simeq 3\times(Mean-Median)$$
如图所示：
![[Pasted image 20240304102748.png|650]]
### 离散程度
> [!definition] 标准差 (Standard Deviation)
> 是方差的平方根，即$\sigma$。

无论数据如何分布，至少有$1-\frac{1}{k^2}$的点在$Mean\pm k\sigma$之内
> [!definition] 四分位数 (Quartile)
> - $Q_1$：下四分位数，25%
> - $Q_3$：上四分位数，75%
> - $IQR$：四分位距，$IQR=Q_3-Q_1$
### 数据统计常用图
> [!definition] 箱形图 (Boxplot)
> ![[Pasted image 20240227165111.png|450]]
> - **下限(Min)**：$Q_1 - 1.5*IQR$
> - **上限(Max)**：$Q_3 + 1.5*IQR$
> - **异常值(Outlier)**：落在上下限之外的点

> [!definition] 分位图 (Quantile-Quantile Plot, Q-Q Plot)
> 使用两组数据中的同一位置的分位数作为一个点的横纵坐标。

> [!example] 
> ![[Pasted image 20240304110136.png|450]]
> 如图可知，Branch 1的价格更低。
## 大数定律和中心极限定理
> [!definition] 马尔可夫不等式 (Markov's Inequality)
> 对于非负随机变量$X$，$\forall c>0$：
> $$P(X\geq c)\leq\frac{\mathbb{E}[X]}{c}$$

> [!proof] 
> $$
> \begin{aligned}
> \mathbb{E}[X]&=\int_0^cxf(x)\ dx+\int_c^\infty xf(x)\ dx \\
> &\geq\int_c^\infty xf(x)\ dx \\
> &\geq c\int_c^\infty f(x)\ dx \\
> &=c\cdot P(X\geq c)
> \end{aligned}
> $$

> [!definition] 切比雪夫不等式 (Chebyshev's Inequality)
> 对于$\forall c>0$：
> $$P\left(|X-\mu|\geq c\right)\leq\frac{\sigma^2}{c^2}$$

> [!proof] 
> $$
> \begin{aligned}
> P\left(|X-\mu|\geq c\right)&=P\left((X-\mu)^2\geq c^2\right)\\
> &\leq\frac{\mathbb{E}\left[(X-\mu)^2\right]}{c^2}(\text{Markov's
> inequality})\\
> &=\frac{\sigma^2+\mathbb{E}^2\left[(X-\mu)\right]}{c^2}\\
> &\leq\frac{\sigma^2}{c^2}
> \end{aligned}
> $$

> [!definition] 切尔诺夫界 (Chernoff Bound)
> 设$X_1,...,X_t$是一串在$[0,1]$范围内独立同分布的随机变量，其期望值为$\mu$，则对于这$t$个变量的均值$X=\frac{1}{t}\sum_iX_i$，当$0<\delta<1$时，有
> $$P(|X-\mu|\geq\delta\mu)\leq2\exp\left(-\frac{\delta^2\mu t}{3}\right)$$
