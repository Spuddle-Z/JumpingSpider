---
tags:
  - Knowledge
aliases:
  - Probability Theory
---
## 随机事件和概率
### 基本概念
> [!definition] 样本空间
> 一次试验的所有可能结果。

> [!definition] 古典概型
> 1. **有限性**：样本空间中只有有限个基本事件；
> 2. **等可能性**：每个基本事件的发生概率相同。
> 
> 符合以上两个特征的试验，称为古典概型。

> [!example] 分组问题
> 将$n$个不同元素分成$m$个不同的组，各组分别有$k_1,...,k_m$个元素，$\sum_{i=1}^{n}k_i=n$，不同的分法有$C_n^{k_1}C_{n-k_1}^{k_2}\cdots C_{k_{m}^{k_{m}}}$种。

> [!definition] 加法定理
> 对任意事件$A,B$，有$P(A\cup B)=P(A)+P(B)-P(AB)$。

加法定理的推广：
$$P\left(\bigcup_{i=1}^{n}A_i\right)=\sum_{i=1}^{n}P(A_i)-\sum_{1\leq i<j\leq n}P(A_iA_j)+\sum_{1\leq i<j<k\leq n}P(A_iA_jA_k)+\cdots+(-1)^{n-1}P(A_1A_2\cdots A_n)$$

### 条件概率
在$A$发生的条件下，发生事件$B$，记作事件$B|A$。则易证
$$P(B|A)=\frac{P(AB)}{P(A)}$$

## 随机变量及其分布
### 离散型随机变量极其分布
> [!definition] Bernoulli概型
> 符合下面条件的试验所对应的概型：
> - 每次试验结果只有两个；
> - 每次试验结果相互独立。

> [!definition] 二项分布
> 令$X$为$n$次试验中事件$A$的发生次数，单次试验中$A$的发生概率为$p$，则$X$服从参数为$n,p$的二项分布，即$X\sim B(n,p)$，且有
> $$P(X=k)=C_n^{k}p^{k}(1-p)^{n-k}$$

> [!definition] Poisson分布
> 令随机变量$X$的样本空间为$\mathbb{N}$，则下面分布律称为服从参数$\lambda$的Poisson分布，记为$X\sim P(\lambda)$：
> $$P(X=k)=e^{-\lambda}\frac{\lambda^{k}}{k!}$$
> ![[Pasted image 20240906145826.png|275]]

> [!theorem] Poisson定理
> 在二项分布中，令$n$变大的同时$p_n$随之变小，且满足$\lim_{n\to\infty}np_n=\lambda>0$，则有
> $$\lim_{n\to\infty}C_n^{k}p_n^{k}(1-p_n)^{n-k}=e^{-\lambda}\frac{\lambda^{k}}{k!}$$
> 即二项分布的极限分布为Poisson分布。

### 连续型随机变量及其分布
> [!definition] 均匀分布
> 若$X$的概率密度函数为
> $$
> f(x)=
> \left\{\begin{aligned}
> &\frac{1}{b-a}&,a<x<b\\
> &0&,\text{otherwise}
> \end{aligned}\right.
> $$
> 则称$X$服从$(a,b)$上的均匀分布，记作$X\sim U(a,b)$。

> [!definition] 指数分布
> 若$X$的概率密度函数为
> $$
> f(x)=
> \left\{\begin{aligned}
> &\lambda e^{-\lambda x}&,x>0\\
> &0&,x\leq0
> \end{aligned}\right.
> $$
> 则称$X$服从$\lambda$的指数分布，记作$X\sim E(\lambda)$。

> [!definition] 正态分布
> 若$X$的概率密度函数为
> $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\quad-\infty<x<+\infty$$
> 则称$X$服从参数为$\mu,\sigma$的正态分布，记作$X\sim N(\mu,\sigma^2)$。其中$\left\{\begin{aligned}\mu=0\\\sigma=1\end{aligned}\right.$的正态分布为**标准正态分布**。

标准正态分布的分布函数记为$\Phi(x)$。

一般正态分布概率可以通过转化成标准正态分布计算。设$X\sim N(\mu,\sigma^2)$的分布函数为$F(x)$，则有
$$F(x)=\Phi\left( \frac{x-\mu}{\sigma} \right)$$

> [!definition] 瑞利分布 (Rayleigh Distribution)
> ![[Pasted image 20240908145234.png|325]]
> 对于一个二维向量$(x,y)$，当$x,y$遵循独立、均值为零、方差相等的正态分布时，此向量模长呈瑞利分布，其概率密度为
> $$f(x)=\frac{x}{\sigma^2}e^{-\frac{x^2}{2\sigma^2}},\quad x>0$$

### 随机变量函数的分布
对于*离散型随机变量*$Y=g(X)$，一般是先求出$Y$的所有可能取值，再确定其每个取值下的概率。

对于*连续型随机变量*$Y=g(X)$，先根据下面的连等式求出$F_{Y}(y)$：
$$F_{Y}(y)=P(Y\leq y)=P(g(X)\leq y)=P(X\in\Omega)=\int_{\Omega}f_{X}(x)\ dx$$
其中$\Omega$即为符合$g(X)\leq y$的$X$取值范围；得到$Y$的分布函数$F_{Y}(y)$后，再对其求导得到$Y$的概率密度函数$f_{Y}(y)=\frac{d}{dy}F_{Y}(y)$。我们将两式结合，即得到**公式法**：
$$f_{Y}(y)=\int_{\Omega}\ \frac{df_{X}(x)}{\frac{dy}{dx}}=\sum_{i=1}^{n}\frac{f_{X}(x_i)}{\left\lvert\frac{dy}{dx}\right\rvert_{x=x_i}}$$
其中$g(x_1)=...=g(x_n)$。

### 多维随机变量的分布
#### 二维随机变量的条件分布
以二维随机变量的条件分布为例，在$\{X=x\}$条件下，$Y$的条件概率密度函数为
$$f_{Y|X}(y|x)=\frac{f(x,y)}{f_{X}(x)}$$
在此基础上求积分，即得到条件分布函数
$$F_{Y|X}(y|x)=\int_{-\infty}^{y}\frac{f(x,t)}{f_{X}(x)}\ dt$$

#### 随机变量的独立性
> [!definition] 独立性
> 以二维随机变量为例，若对于$\forall x,y$，都有
> $$P(X\leq x,Y\leq y)=P(X\leq x)\cdot P(Y\leq y)$$
> 则称$X,Y$相互独立。

## 随机变量的数字特征
### 期望与方差
> [!definition] 期望 (Expectation)
> 度量一个随机变量取值的集中位置或平均水平的最基本的数字特征。

期望的运算性质：
1. $E(cX)=cE(X)$
2. $E(X\pm Y)=E(X)\pm E(Y)$
3. 若$X,Y$相互独立，则有$E(XY)=E(X)E(Y)$

> [!definition] 方差 (Variance)
> 表示随机变量取值的分散性的一个数字特征，一般用$\sigma^2$或$D(X)$表示，可以由期望按下式算得
> $$D(X)=E[(X-E(X))^2]=E(X^2)-E(X)^2$$

方差的运算性质：
1. $D(c)=0$
2. $D(cX)=c^2D(X)$
3. $D(X+c)=D(X)$
4. 若$X,Y$相互独立，则有$D(X\pm Y)=D(X)+D(Y)$

下面是一些重要随机变量的数学期望和方差：

| 分布                     | 分布律或概率密度                                                                                                        | 期望                    | 方差                      |
| ---------------------- | --------------------------------------------------------------------------------------------------------------- | --------------------- | ----------------------- |
| 二项分布 $B(n,p)$          | $$P(X=k)=C_n^{k}p^{k}(1-p)^{n-k}$$                                                                              | $$n$$                 | $$p$$                   |
| 泊松分布 $P(\lambda)$      | $$P(X=k)=\frac{\lambda^{k}}{k!}e^{-\lambda}$$                                                                   | $$\lambda$$           | $$\lambda$$             |
| 均匀分布 $U(a,b)$          | $$f(x)=\left\{\begin{aligned}\frac{1}{b-a},&\quad a<x<b\\0,&\quad \text{otherwise}\end{aligned}\right.$$        | $$\frac{a+b}{2}$$     | $$\frac{(b-a)^2}{12}$$  |
| 指数分布 $E(\lambda)$      | $$f(x)=\left\{\begin{aligned}\lambda e^{-\lambda x},&\quad x>0\\0,&\quad \text{otherwise}\end{aligned}\right.$$ | $$\frac{1}{\lambda}$$ | $$\frac{1}{\lambda^2}$$ |
| 正态分布 $N(\mu,\sigma^2)$ | $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$                                            | $$\mu$$               | $$\sigma^2$$            |

### 协方差与相关系数
> [!definition] 协方差 (Covariance)
> 用来衡量两个随机变量的变化趋势是否一致。对于两个随机变量$X,Y$，其协方差为$$\text{cov}(X,Y)=E\left[(X-E(X))\cdot(Y-E(Y))\right]=E(XY)-E(X)E(Y)$$
> 协方差也可记为$\sigma(X,Y)$。

从数值上来说，协方差越大，说明两个变量的同向趋势越大；协方差越小，说明两个变量的反向趋势越大；协方差越趋近于零，两个变量越不相关。

此外，协方差还遵循
$$D(X\pm Y)=D(X)+D(Y)\pm2\text{cov}(X,Y)$$

> [!definition] 相关系数
> 相关系数是标准化后的协方差，不依赖于随机变量的量纲。当$D(X),D(Y)>0$时，$X,Y$的相关系数为
> $$\rho_{XY}=\frac{\text{cov(X,Y)}}{\sqrt{D(X)\cdot D(Y)}}$$

> [!definition] 相关性
> 当$\rho_{XY}=0$时，随机变量$X,Y$不相关。

> [!definition] 协方差矩阵
> 对于$d$个随机变量，我们可以求出一个协方差矩阵：
> $$
> \Sigma=
> \begin{bmatrix}
> \sigma(X_1,X_1) & \cdots & \sigma(X_1,X_d) \\
> \vdots & \ddots & \vdots \\
> \sigma(X_d,X_1) & \cdots & \sigma(X_d,X_d)
> \end{bmatrix}
> $$
> ^yvq7oj

### 运算规律
若$a,b$为两个*相互独立*的随机变量，则有
$$
\left\{\begin{aligned}
a\sim N(\mu_1,\sigma_1^2)\\
b\sim N(\mu_2,\sigma_2^2)
\end{aligned}\right.
\implies a+b\sim N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)
$$
对于两个向量$\alpha,\beta$，则有
$$
\left\{\begin{aligned}
\alpha\sim N(\mu_{\alpha},\sigma_{\alpha}^2)\\
\beta\sim N(\mu_{\beta},\sigma_{\beta}^2)
\end{aligned}\right.
\implies
\begin{bmatrix}
\alpha \\ \beta
\end{bmatrix}
\sim N\left(
\begin{bmatrix}
\mu_{\alpha}\\\mu_{\beta}
\end{bmatrix},
\begin{bmatrix}
\Sigma_{\alpha\alpha}&\Sigma_{\alpha\beta}\\
\Sigma_{\beta\alpha}&\Sigma_{\beta\beta}
\end{bmatrix}
\right)
$$
且有
$$P(\beta|\alpha)\sim N(\mu_{\beta}+\Sigma_{\beta\alpha}\Sigma_{\alpha\alpha}^{-1}(\alpha-\mu_{\alpha}),\Sigma_{\beta\beta}-\Sigma_{\beta\alpha}\Sigma_{\alpha\alpha}^{-1}\Sigma_{\alpha\beta})$$

## 大数定律和中心极限定理
### 预备知识
> [!definition] 马尔可夫不等式 (Markov's Inequality)
> 对于非负随机变量$X$，$\forall c>0$：
> $$P(X\geq c)\leq\frac{\mathbb{E}[X]}{c}$$

> [!proof] 
> $$
> \begin{aligned}
> \mathbb{E}[X]&=\int_0^cxf(x)\ dx+\int_c^\infty xf(x)\ dx \\
> &\geq\int_c^\infty xf(x)\ dx \\
> &\geq c\int_c^\infty f(x)\ dx \\
> &=c\cdot P(X\geq c)
> \end{aligned}
> $$

> [!definition] 切比雪夫不等式 (Chebyshev's Inequality)
> 对于$\forall c>0$：
> $$P\left(|X-\mu|\geq c\right)\leq\frac{\sigma^2}{c^2}$$

> [!proof] 
> $$
> \begin{aligned}
> P\left(|X-\mu|\geq c\right)&=P\left((X-\mu)^2\geq c^2\right)\\
> &\leq\frac{\mathbb{E}\left[(X-\mu)^2\right]}{c^2}\qquad(\text{Markov's
> inequality})\\
> &=\frac{\sigma^2+\mathbb{E}^2\left[(X-\mu)\right]}{c^2}\\
> &\leq\frac{\sigma^2}{c^2}
> \end{aligned}
> $$

> [!definition] 切尔诺夫界 (Chernoff Bound)
> 设$X_1,...,X_t$是一串在$[0,1]$范围内独立同分布的随机变量，其期望值为$\mu$，则对于这$t$个变量的均值$X=\frac{1}{t}\sum_iX_i$，当$0<\delta<1$时，有
> $$P(|X-\mu|\geq\delta\mu)\leq2\exp\left(-\frac{\delta^2\mu t}{3}\right)$$

### 大数定律
> [!definition] 大数定律
> 若随机变量序列$X_1,X_2,...,X_{n},...$满足：对于$\forall\varepsilon>0$，有
> $$\lim_{n\to\infty}P\left( \left\lvert\frac{1}{n}\sum_{i=1}^{n}X_i-\frac{1}{n}\sum_{i=1}^{n}E(X_i)\right\rvert<\varepsilon\right)=1$$
> 则该序列服从大数定律。

> [!definition] 切比雪夫大数定律
> 若随机变量序列$X_1,X_2,...,X_{n},...$满足：
> - 其中的随机变量互不相关
> - 方差存在
> - $\forall X_i$的方差都小于一个固定常数
> 
> 则该序列服从大数定律。

> [!definition] 辛钦大数定律
> 若随机变量序列$X_1,X_2,...,X_{n},...$满足：
> - 所有随机变量独立同分布
> - $E(X_i)$存在
> 
> 则该序列服从大数定律。

### 中心极限定理
> [!definition] 中心极限定理
> 若随机变量序列$X_1,X_2,...,X_{n},...$是独立同分布的，且其数学期望与方差都存在，则对于$x\in\mathbb{R}$，有
> $$\lim_{n\to\infty}P\left(\frac{\sum_{i=1}^{n}X_i-n\mu}{\sqrt{n}\sigma}\leq x\right)=\Phi(x)$$
> 我们令$Y_n=\frac{\sum_{i=1}^{n}X_i-n\mu}{\sqrt{n}\sigma}$，$Y_n$即为$\sum_{i=1}^{n}X_i$标准化后的随机变量，原定理即表示$n$足够大时，$Y_n$近似于标准正态分布。

## 数理统计
### 常用统计量的分布
> [!definition] $\chi^2$分布
> ![[Pasted image 20240912214205.png|450]]
> 设随机变量$X_1,X_2,...,X_n$相互独立，且$X_i\sim N(0,1)$，则$\sum_{i=1}^{n}X_i^2$服从自由度为$n$的$\chi^2$分布，记为$\sum_{i=1}^{n}X_i^2\sim\chi^2(n)$。

$\chi^2$分布有如下性质：
- 对于$X\sim\chi^2(n)$，$E(X)=n$，$D(X)=2n$；
- 对于相互独立的两个随机变量$X_1\sim\chi^2(n_1),X_2\sim\chi^2(n_2)$，有$X_1+X_2\sim\chi^2(n_1+n_2)$

> [!definition] $t$分布
> 对于相互独立的两个随机变量$X\sim N(0,1),Y\sim\chi^2(n)$，$T=\frac{X}{\sqrt{Y/n}}$服从自由度为$n$的$t$分布，记为$T\sim t(n)$。

使用$t_{\alpha}(n)$表示自由度$n$的$t$分布的上侧$\alpha$分位数，且有
$$t_{1-\alpha}(n)=-t_{\alpha}(n)$$

> [!definition] $F$分布
> ![[Pasted image 20240912222058.png|450]]
> 对于相互独立的两个随机变量$U\sim\chi^2(m),V\sim\chi^2(n)$，$F=\frac{U/m}{V/n}$服从第一自由度$m$，第二自由度$n$的$F$分布，记为$F\sim F(m,n)$。

$F$分布有如下性质：
- 若$F\sim F(m,n)$，则$\frac{1}{F}\sim F(n,m)$；
- $F(m,n)$的上侧$\alpha$分位数记为$F_{\alpha}(m,n)$，则有
	$$F_{1-\alpha}(m,n)=\frac{1}{F_{\alpha}(n,m)}$$

> [!theorem] Fisher定理
> 设$X\sim N(\mu,\sigma^2)$，而$(X_1,X_2,...,X_{n})$是来自总体$X$的一个简单随机样本，$\bar{X},S^2$分别是样本均值与样本方差，则
> - $\bar{X}\sim N\left( \mu,\frac{\sigma^2}{n} \right)$
> - $\frac{(n-1)S^2}{\sigma^2}\sim\chi^2(n-1)$
> - $\frac{(n-1)S^2}{\sigma^2}$与$\bar{X}$相互独立

由以上定理可得到以下推论：
$$\frac{\bar{X}-\mu}{S/\sqrt{n}}\sim t(n-1)$$

## 参数估计
### 基本概念
参数估计是指根据随机抽样出的样本，来估计总体分布的参数，具体可以进一步分为**点估计**和**区间估计**。
- 点估计估计出来的参数就是一个值；
- 而区间估计估计出的参数是一个近似值，有一个误差范围，这使得我们能够由此知道这个估计的可靠程度。

### 点估计
#### 矩估计
> [!definition] 矩 (Moment)
> - **原点矩**：随机变量$X$的$k$阶原点矩即为$E(X^{k})$；
> - **中心矩**：随机变量$X$的$k$阶中心矩即为$E([X-E(X)]^{k})$，易得其二阶中心矩即为方差$D(X)$。

> [!definition] 矩估计法
> 求出样本矩，直接将其当作总体矩，这种估计方法就是矩估计法。

#### 最大似然估计
> [!definition] 似然函数 (Likelihood Function)
> 对于函数$P(x|\theta)$，其中$x$表示实际样本数据，$\theta$表示模型参数，则若$x$固定，$P(x|\theta)$随$\theta$变化，那么这个函数叫做似然函数。

> [!note] 似然函数与概率函数
> 若$\theta$固定，$P(x|\theta)$随着$x$变化，则此函数是一个固定模型的概率函数。

> [!definition] 最大似然估计 (Maximum Likelihood Estimation, MLE)
> 当我们有一组数据$x_1,x_2,\cdots,x_{n}$时，我们需要求出那个最可能得出这组数据的模型参数$\theta^*$，也即求$$\mathop{\arg\max}\limits_{\theta}\prod_{i=1}^{n}P(x_i|\theta)\equiv\mathop{\arg\max}\limits_{\theta}\sum_{i=1}^{n}\log P(x_i|\theta)$$ ^tyfz0r

#### 估计量的评价标准
> [!definition] 无偏性
> 设参数$\theta$的估计量为$\hat{\theta}$，则若
> $$E(\hat{\theta})=\theta$$
> 即用样本估计出来的参数的期望就是此参数的真实值，则称$\hat{\theta}$为$\theta$的**无偏估计量**。

> [!example] 有偏估计量的例子
> 方差$\sigma^2$的无偏估计量是
> $$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2$$
> 可以注意到加和的时候用的是$n$个样本，但前面的系数并不是$\frac{1}{n}$，而是$\frac{1}{n-1}$，因为样本方差并不是实际方差的无偏估计量，样本方差会偏小，这可以很容易地用无偏性的定义严格证明。
> 
> 直观上解释的话，就是采样的时候，概率较小的地方可能会根本没采到样本，这就使样本的分布更加趋于密集，使方差变小。

> [!definition] 有效性
> 若$\hat{\theta}_1,\hat{\theta}_2$均为$\theta$的无偏估计量，那么若
> $$D(\hat{\theta}_1)<D(\hat{\theta}_2)$$
> 则称$\hat{\theta}_1$比$\hat{\theta}_2$有效。

### 区间估计
> [!definition] 置信度与置信区间
> ![[图片1.png|300]]
> 置信度为$1-\alpha$的置信区间$(\hat{\theta}_1,\hat{\theta}_2)$是指参数$\theta\in(\hat{\theta}_1,\hat{\theta}_2)$的概率为$1-\alpha$。上图即为置信度$95\%$的示意图。

若总体为$X\sim N(\mu,\sigma^2)$，从中取出的一个样本为$(X_1,X_2,...,X_{n})$，样本均值与样本方差分别为$\bar{X},S^2$，要求置信度为$1-\alpha$的置信区间。
- 若求均值$\mu$的置信区间
	- 若方差$\sigma^2$已知，则置信区间为
		$$\left(\bar{X}-u_{\alpha/2}\frac{\sigma}{\sqrt{n}},\bar{X}+u_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)$$
	- 若方差$\sigma^2$未知，则置信区间为
		$$\left(\bar{X}-t_{\alpha/2}(n-1)\frac{S}{\sqrt{n}},\bar{X}-t_{\alpha/2}(n-1)\frac{S}{\sqrt{n}}\right)$$
- 求方差$\sigma^2$等的置信区间在此省略，我觉得暂时用不上。

## 假设检验
假设检验即通过样本数据的分布特征，判断总体是否符合假设的分布。

假设检验的步骤如下：
1. 我们先有一个事先确定的，对数据分布的假设，称为**原假设**（原假设的对立面称为**备择假设**），比如说假设原数据的期望$\mu=\mu_0$，那么备择假设就是$\mu\neq\mu_0$；
2. 根据原假设，我们可以用[[概率论#区间估计|区间估计]]中的内容得到一个置信度为$1-\alpha$的置信区间；
3. 此时进行采样，然后根据样本使用[[概率论#点估计|点估计]]得到一个估计的期望$\hat{\mu}$，如果$\hat{\mu}$落在了置信区间之外，按照原假设$\mu=\mu_0$，这种情况出现的概率只有$\alpha$，是一个小概率事件，此时我们拒绝原假设，反之则接受。

上述步骤中的$\alpha$称为**显著性水平**。一般当$\alpha=0.05$时，拒绝原假设称为是**显著**的；当$\alpha=0.01$时，拒绝原假设称为是**高度显著**的。换句话说，拒绝$\alpha=0.05$时的原假设，说明只有5%的概率原假设靠谱，就是说显著地不靠谱。

## 其它实用统计知识
### Logit
> [!definition] Odds
> 是用来表示概率大小的量，不过概率的取值范围为$[0,1]$，而odds的取值范围为$[0,+\infty)$，其公式为
> $$Odds(P)=\frac{P}{1-P}$$

> [!definition] Logit函数
> 在odds的基础上取对数，即可得到Logit
> $$Logit(P)=\log Odds(P)=\log\frac{P}{1-P}$$
> 其图像如下：
> ![[Pasted image 20240317154521.png|400]]

> [!definition] Sigmoid函数
> 是Logit的反函数
> $$\sigma(x)=\frac{1}{1+e^{-x}}$$
> ![[Pasted image 20240317155319.png|400]]

### 数据的基本统计描述
#### 中心趋势
对于平均数(Mean)、中位数(Median)与众数(Mode)，在数据分布为*单峰*时有此经验公式：
$$Mean-Mode\simeq 3\times(Mean-Median)$$
如图所示：
![[Pasted image 20240304102748.png|650]]
#### 离散程度
> [!definition] 标准差 (Standard Deviation)
> 是方差的平方根，即$\sigma$。

无论数据如何分布，至少有$1-\frac{1}{k^2}$的点在$Mean\pm k\sigma$之内
> [!definition] 四分位数 (Quartile)
> - $Q_1$：下四分位数，25%
> - $Q_3$：上四分位数，75%
> - $IQR$：四分位距，$IQR=Q_3-Q_1$
#### 数据统计常用图
> [!definition] 箱形图 (Boxplot)
> ![[Pasted image 20240227165111.png|450]]
> - **下限(Min)**：$Q_1 - 1.5*IQR$
> - **上限(Max)**：$Q_3 + 1.5*IQR$
> - **异常值(Outlier)**：落在上下限之外的点

> [!definition] 分位图 (Quantile-Quantile Plot, Q-Q Plot)
> 使用两组数据中的同一位置的分位数作为一个点的横纵坐标。

> [!example] 
> ![[Pasted image 20240304110136.png|450]]
> 如图可知，Branch 1的价格更低。
