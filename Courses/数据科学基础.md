---
tags:
  - Knowledge
  - Code
aliases:
  - Foundations of Data Science
---
## 统计学基础
### 相似性与差异性
> [!definition] 闵可夫斯基距离 (Minkowski Distance)
> 有两向量$x=(x_1,x_2,...,x_n),y=(y_1,y_2,...,y_n)$，则闵可夫斯基距离表示为
> $$D=\left(\sum_{i=1}^n|x_i-y_i|^p\right)^{\frac{1}{p}}$$

> [!definition] 余弦距离 (Cosine Similarity)
> 对于两个向量$x,y$，其余弦相似度就是其夹角的余弦值：
> $$\cos(x,y)=\frac{x\cdot y}{||x||\cdot||y||}$$
> 两向量夹角越小，越相似。

> [!definition] 杰卡德距离 (Jaccard Distance)
> 对于两个集合$A,B$，*Jaccard相似系数*$J(A,B)=\frac{|A\cup B|}{|A\cap B|}$用来描述两个集合的相似度，而Jaccard距离则用于度量两个集合的差异性：
> $$d_{J}(A,B)=1-J(A,B)=1-\frac{|A\cup B|}{|A\cap B|}$$
## MapReduce
### 基本概念
**MapReduce**是一种编程思想，即把大的问题细分为小的问题，然后再分别调度、处理。

类比成一个很大的饭店为许多食客做菜的过程：数据——食材，简单重复但大量的任务——切菜、烧水，复杂任务——烹饪。
![[Pasted image 20240305161123.png|475]]
如上图，我们可以将制作一道菜的完整流程分交给不同的人做。
### 工作原理
- 将最原始的数据抽象成key-value的形式，我们将数据处理过程抽象成两个步骤，即map与reduce；
- Map：将相同key的数据聚合起来；
- Reduce：分别处理这些key不同的数据，并输出。
![[Pasted image 20240305163846.png]]
- 例：自然连接两个表格![[Pasted image 20240305170543.png|500]]
	- Map：将$b$提取为key，将$R$中的每个元组变为$(b,(a,R))$的形式，$S$同理；
	- Reduce：将$b$相同但来自不同表格的key-value对进行合并，并输出成$(a,c)$的形式。

## LSH
### 定义
> [!definition] 局部敏感哈希 (Locality Sensitive Hashing, LSH)
> LSH是一种近似搜索算法。对于一个拥有海量数据的数据集，通过一一比对来进行搜索是不现实的。近似搜索即只搜索与目标比较接近的一部分数据，大大降低了搜索量。
### 具体过程
#### 概述
如下图，LSH算法主要分为三个步骤：*Shingling*、*MinHashing*与*Banding*。
![[Pasted image 20240329151102.png|500]]
#### Shingling
- **Shingling**：将文件转化成token串的集合，$k$-shingle则为长度为$k$的token串（如令$k=2$，文件$D=abcab$的$2$-shingles为$S(D)=\{ab,bc,ca\}$）。

有了shingle集，我们就可以根据它创建一个one-hot编码。
![[Pasted image 20240329151418.png|500]]
#### MinHashing
- **最小哈希(MinHashing)**：将上一步得到的稀疏的one-hot向量转换为稠密的**签名(Signature)**。

假设我们现在要将一个*6维的one-hot*转化为一个*4维签名*，则其步骤如下：
![[minHash 1.gif|500]]
1. 生成4个长度为6的递增序列，并随机打乱，作为MinHash函数；
2. 从1开始依次检查one-hot对应位置；
3. 若对应位置为0，则继续检查下一个数字；
4. 若为1，则返回当前的数字作为此MinHash函数的输出。

![[Pasted image 20240329153939.png|500]]
可以证明信息的相似性在MinHash操作的前后得到了保留。
#### Banding
有了签名之后，我们有两种思路去比较两个签名的相似度。
![[Pasted image 20240401204920.png|375]]
- 一种是AND的思路，即只有两个签名的每一位都相等，二者的哈希才会相同，如图中*蓝线*；
- 另一种是OR的思路，即只要两个签名中的某一位相等，我们便认为二者相等，如图中*绿线*。

但我们希望的是如图中*红线*一样，在相似到某个程度之后，才被分到一个**哈希桶(Hash Bucket)**中，此时便需要结合两种思路的特点。

- **Banding**：将整个的签名拆分成子向量，在子向量中使用AND思路，在子向量间使用OR思路，即两个签名中只要有一个子向量相等，便将这两个签名记为一个候选对。![[Pasted image 20240329160309.png|600]]
### 代码示例
```python
import pandas as pd
import numpy as np
from tqdm import tqdm

# Load data
'''
此处的数据是200个文件，每个文件都已经被转化成了一个长度为1e6的one-hot向量
'''
df = pd.read_csv("/kaggle/input/docs-for-lsh/docs_for_lsh.csv")
data = df.iloc[:, 1:].to_numpy()

# Set parameters
v = 1000000    # Length of one-hot
f = 200        # Numbers of file
n = 100        # Length of signature
b = 20         # Numbers of bands
r = n // b     # Rows per band

# 创建n个哈希函数，n为签名的长度
np.random.seed(n)
hashes = np.array([np.random.permutation(v) for _ in tqdm(range(n))])
print("Hashes:", hashes.shape)

# 根据创建出的哈希函数与one-hot向量算出签名
def minHash(hashes, one_hot):
    signature = np.full(hashes.shape[0], np.inf)
    for i, flag in enumerate(one_hot):
        if flag:
            signature = np.minimum(signature, hashes[:, i])
    return signature.tolist()

signs = np.array([minHash(hashes, data[:,col]) for col in tqdm(range(200))])
print("Signs:", signs.shape)

import hashlib

# 创建哈希桶
buckets = {}
for file in tqdm(range(n)):
    for band in range(b):
        hashObj = hashlib.md5()
        begin = file * r
        tmp = str(signs[file][begin:begin+r])
        hashObj.update(tmp.encode())
        tag = hashObj.hexdigest()
        if tag not in buckets:
            buckets[tag] = [file]
        elif file not in buckets[tag]:
            buckets[tag].append(file)

# 找到与0号文件在同一个桶中的文件，只计算这些文件与0号文件的相似性
similar_docs = {}
for bucket in buckets.values():
    if 0 in bucket:
        for doc_id in bucket:
            if doc_id != 0 and doc_id not in similar_docs:
                similarity = jaccard_score(data[0], data[doc_id])
                similar_docs[doc_id] = similarity

# 按照相似度排序，并输出前30位
sorted_similar_docs = sorted(similar_docs.items(), key=lambda x: x[1], reverse=True)

top_30 = sorted_similar_docs[:30]

for doc_id, similarity in top_30:
    print(f"Document ID: {doc_id}  \tSimilarity: {similarity}")
```

## PageRank
### 基本概念
**PageRank**是Google的看家算法，其可以对海量的网页进行重要性分析，并按照其重要性排序。
### 随机游走模型
将整个互联网看作一个巨大的有向图，每个网页都是一个节点，而超链接则是从一个页面指向另一个页面的有向边。在以上假设下，我们引入随机游走模型：
> [!definition] 随机游走 (Random Walk)
> 在一个图中，我们假设一个浏览者(Surfer)会以某个节点为起点，按照相同的概率，随机跳转到其相邻节点，这个过程称为随机游走。 ^rpjv8d

其实从任意一个点开始随机游走，那么在一个无限长的时间内，这个浏览者在所有节点出现的概率会趋于稳定。此时，浏览者出现在每个节点上的概率即为这个节点对应网页的PageRank值。

因此，我们可以在此基础上建立一个流模型。可以将每条边上的权重看作概率，则每个节点出链的权重和为1，出链则平均分配这些权重，如图：
![[Pasted image 20240415201656.png|175]]
我们设节点$v$的PageRank值为$PR(v)$，出度为$L(v)$，所有指向$v$的节点集合为$B(v)$，节点总数为$N$，则第$t$次迭代的PR值为：
$$PR_{t}(v)=\sum_{u\in B(v)}\frac{PR_{t-1}(u)}{L(u)}$$
这样一个线性变换可以使用一个转移矩阵$M$表达：
$$PR_{t}=M\cdot PR_{t-1}$$
除去两种特殊的情况，经过多次迭代后，PR值最终会收敛，即使得$PR_{t}=PR_{t-1}$。因此其实最终收敛到的PR值就是$M$特征值为1的特征向量，可以由此直接计算出PR值。
### 存在的问题及解决方法
1. 当这个网页组成的图不是[[图论#^dn9bhk|强连通图]]时，会出现如下图的情况，即在多次迭代之后，红色与蓝色点的PR值将变为零（蓝色点释放掉了一些权重，因为其没有出度；绿色点则吸收了剩下的权重）。
	![[Pasted image 20240415204139.png|145]]
1. 绿色部分还会存在周期性的问题，即权重会在里面打转，导致无法达到平衡。

以上两种情况可以通过随机传送来解决：
> [!definition] 随机传送 (Ramdom Teleports)
> 引入一个概率$\beta$，使得每个节点上都有$1-\beta$的概率传送到一个随机的节点上。

因此，谷歌使用的矩阵为
$$A=\beta M+(1-\beta)\left[ \frac{1}{N} \right]_{N\times N}$$
### Code
```python
import networkx as nx

# 加载数据
G_tmp = nx.read_edgelist('../input/google-web-graph/web-Google.txt', create_using = nx.DiGraph)
# 留下最大的弱连通分量
G = max(nx.weakly_connected_components(G_tmp), key=len)
# 打印图的一些信息
print(nx.info(G))
# 直接调用PageRank算法
pagerank_nx = nx.pagerank(G, alpha=0.85, tol=1e-10)
```
