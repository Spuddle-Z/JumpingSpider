---
tags:
  - Knowledge
aliases:
  - Natural Language Processing
  - NLP
---
## NLP基础
### TF-IDF
> [!definition] 倒排
> 倒排是指提前遍历所有文档，存储出现的关键词，搜索时直接根据关键词进行搜索。其复杂度较遍历所有文档大大降低。

> [!definition] 词频 (Term Frequency)
> 表示某个关键词在文档中出现的频率，其计算公式如下：
> $$tf_{i,j}=\frac{n_{i,j}}{\sum_kn_{k,j}}$$
> 其中$n_{i,j}$表示关键词$w_i$在文档$d_j$中出现的次数。

> [!definition] 逆向文档频率 (Inverse Document Frequency)
> 逆向文档频率负责降低常用词的权重，如“的”、“是”等。其计算公式如下：
> $$idf_i=\log\frac{|D|}{|\{j:w_i\in d_j\}|}$$
> 其中$|D|$表示文档总数，$|\{j:w_i\in d_j\}|$表示包含关键词$w_i$的文档数。

> [!definition] TF-IDF
> TF-IDF是用于信息检索与搜索引擎上的一种统计模型，通过公式$tf-idf_{i,j}=tf_{i,j}\times idf_i$来得到关键词$w_i$在文档$d_j$中的权重，此值越大匹配越好。

### 评价语言模型性能
给出一段*足够长的*、*真实的*、*模型没有见过*的语料，则模型对这段语料预测的概率越高，则说明这个模型越好。
- 足够长：避免模型“运气好”，刚好在要预测的几个位置上表现良好；
- 真实：只有在真实语料上模型预测的概率才有意义；
- 模型没有见过：避免模型背下来整个训练集，然后测试的时候直接默写。

> [!definition] 混淆度 (Perplexity)
> 混淆度指在测试集中，平均每个模型做出预测的位置上，那个实际出现的真实词被模型赋予的概率的倒数。对于一个由$N$个单词组成的语料，模型给出此测试集的概率为$\prod_{i=1}^NP(w_i|w_1,\cdots,w_{i-1})$，接着对测试集长度归一化（因为是相乘，所以用几何平均）得到$\sqrt[N]{\prod_{i=1}^NP(w_i|w_1,\cdots,w_{i-1})}$，在此基础上取倒数得到下式：
> $$PPW=\frac{1}{\sqrt[N]{\prod_{i=1}^NP(w_i|w_1,\cdots,w_{i-1})}}$$

对于完全没受过训练的语言模型，其$PPW=|V|$，其中$|V|$表示词汇表的大小；而对于一个完美的语言模型，其$PPW=1$。因此，我们希望模型的$PPW$越小越好。

此外，另一种
> [!definition] 双语评估替补 (Bilingual Evaluation Understudy, BLEU)
> 对于生成翻译文本序列的好坏，一般用BLEU来衡量，此值越高，生成序列越好。BLEU的区间为$[0,1]$，可以由$S_1,S_2$两部分相乘得到，$S_1$用于惩罚过短的预测，$S_2$用于奖励更长的匹配：
> $$
> \left\{\begin{aligned}
> BLUE&=S_1\cdot S_2 \\
> S_1&=\exp\left(\min\left\{0,1-\frac{len_{label}}{len_{pred}}\right\}\right) \\
> S_2&=\prod_{i=1}^np_i^{2^{-i}}
> \end{aligned}\right.
> $$
> 其中$p_n$为n-gram的预测精度。

> [!example] 
> 例如对于序列`Today is a nice day`，预测结果为`It is a nice day today`，则此时
> $$p_1=\frac{5}{6},p_2=\frac{3}{5},p_3=\frac{2}{4},p_4=\frac{1}{3},p_5=0$$

### 词性标注
> [!definition] 词性标注
> 就是用算法自动将句子中的每个词标注其语法属性的过程。

我们运用隐马尔可夫模型来进行词性标注。

> [!definition] 隐马尔可夫模型 (Hidden Markov Model, HMM)
> 其模型示意图与公式如下：
> ![[HMM.png|350]]
> $$
> \left\{\begin{aligned}
> \mathop{\arg\max}\limits_hP(h|v)&=\mathop{\arg\max}\limits_h\frac{P(v|h)\cdot P(h)}{P(v)}=\mathop{\arg\max}\limits_hP(v|h)\cdot P(h)\\
> P(h)&=P(h_1)\prod_{i=2}^NP(h_i|h_{i-1})\\
> P(v|h)&=\prod_{i=1}^NP(v_i|h_i)
> \end{aligned}\right.
> $$
> 其中$v$代表一句话中的词，$h$代表其对应的词性。HMM的目标是给定一句话$v$，得到最可能的词性$h$，即求得$\mathop{\arg\max}\limits_hP(h|v)$。一般用对数将连乘转化为连加。

直接使用隐马尔可夫模型需要计算所有的$P(h,v)$，其计算量过于庞大，因此引入维特比算法来简化模型。

> [!definition] 维特比算法 (Viterbi Algorithm)
> Viterbi算法的思想是，顺序计算每个词词性的最大概率，然后在此基础上计算下一个词词性的最大概率，直到计算完所有词的词性。

由于此方法可能遗漏最优解，因此可以使用[[Transformer#^b477c5|束搜索]]对其进行优化。

> [!definition] 前向-后向算法 (Forward-Backward Algorithm)
> 此算法是一种无监督学习算法。

### 中文分词
#### 方法概览
中文分词按照是否使用词典可以分为*有词典切分*和*无词典切分*。有词典切分中基于规则的方法（如最大匹配法与最短路径法）比较机械，无法避免一些较为灵活的歧义；而与之相对的基于统计的方法则可以避免这些问题，但是需要大量的语料进行训练。

> [!definition] 语言模型法
> 语言模型法是一种有词典切分的基于统计的方法。其思想是从头开始预测之后一段文字的数种切分方式，然后引入束搜索的思想来优化模型的计算规模。

#### 条件随机场
> [!definition] 条件随机场 (Conditional Random Field, CRF)
> 条件随机场是将分词问题看作一个词性标注问题的分词方法。具体做法是给每个汉字标注词首(B)、词中(M)、词尾(E)、单字成词(S)四种词性中的一种，而后根据标注来划分单词。

> [!example] 
> 以下图为例：
> ![[CRF_example.png|375]]

CRP标注词性的模型如下：
![[CRF_structure.png|275]]
$$
\left\{\begin{aligned}
\mathop{\arg\max}\limits_hP(h|v)&=\mathop{\arg\max}\limits_h\frac{score(h,v)}{\sum_hscore(h,v)}\\
score(h,v)&=\exp\left(\sum_{k=1}^{K}w_kF_k(h,v)\right)\\
F_k(h,v)&=\sum_{i=2}^{N}f_k(h_{i-1},h_i,v,i)
\end{aligned}\right.
$$
其中$f_k(h_{i-1},h_i,v,i)$一般是值域为$\{0,1\}$的二值函数，但也可以非常灵活，因此其允许我们在某个字预测非常不好时，手动定义此特征函数。

> [!note] 
> 对于一个实际的CRF分词器来说，其$f_k$的数量会在$10^5$\~$10^6$量级，因此需要算法按照特征模板(Feature Template)来生成$f_k$。

CRP的训练是一个监督学习，我们的目标是求出
$$\mathop{\arg\max}\limits_{w}\sum_{s=1}^{M}P(h^s,v^S)$$
为简化计算，我们令$\sum_{h}score(h,v)=Z(v)$，并将似然函数取对数，最终去求
$$\mathop{\arg\min}\limits_w\left[-\log\sum_{s=1}^{M}\frac{1}{Z(v_s)}\exp\left(\sum_{k=1}^{K}\sum_{i=2}^{N}w_kf_k(h_i^s,h_{i-1}^s,v^s,i)\right)\right]$$

我们利用F1-Score来评价模型的性能。

### 英文分词
#### 词级别的词元切分 Word-level Tokenization
英语中单词之间是以空格分隔的，因此基于规则的方法就可以很好地解决英文分词问题，无需动用基于学习的方法。

#### 亚词级别的词元切分 Subword-level Tokenization
英语等语言的词汇数量通常能够达到数十万，且其中那些共享词根的单词联系紧密，因此词并非一个合适的切分单元；而字母的数量常常只有几十个，也不是一个合适的切分单元。

> [!definition] BPE (Byte Pair Encoding)
> BPE是一种基于统计的亚词级别的词元切分方法。其思想是不断将语料中出现频率最高的字母组合成一个新的字母，直到达到预设的词汇量。

## 语法解析
### 构成式语法
#### 基本概念
> [!definition] 构成式语法
> 构成式语法是从一个根节点开始，通过一系列的规则将其分解为若干个子节点，直到所有节点都是终结符的语法。

其三要素为：
- **语法成分 (Constituent)**：语法树中的每个节点；
- **语法规则 (Rule)**：将一个节点分解为若干个子节点的规则；
- **词典 (Lexicon)**：将终结符映射为词汇的词典。

三者关系如下：
![[constituency_grammar.png|550]]

> [!definition] 上下文相关语法 (Context-Sensitive Grammar, CSG)
> 上下文相关语法中语法树的生成与上下文有关，也即其语法规则中$\rightarrow$左边不止单个元素。

> [!definition] 上下文无关语法 (Context-Free Grammar, CFG)
> 与上下文相关语法相反，其语法规则中$\rightarrow$左边只能是单个元素。上下文无关语法由四元组$G=(V_N,V_T,P,S)$组成，其中$V_N$是非终结符集合，$V_T$是终结符集合，$P$是产生式集合，$S$是开始符号。

^6edb2e

> [!definition] 乔姆斯基范式 (Chomsky Normal Form, CNF)
> 乔姆斯基范式则在CFG的基础上，每条语法规则的右边只能是两个元素，且每条词典规则只能从一个元素生成一个单词，这样便使其语法树为一个二叉树。

> [!definition] 概率化的上下文无关语法 (Probabilistic Context-Free Grammar, PCFG)
> 概率化的上下文无关语法则在CNF的基础上，给每条语法规则与词典规则记录一个概率。

#### 分析方法
> [!definition] Cocke-Younger-Kasami算法 (CYK Algorithm)
> CYK算法是一种自底向上的动态规划算法，用于判断一个句子是否符合某个CNF语法。

> [!example] 
> 以句子"She eats a fish with a fork"为例，作一阶梯状表格，若某个格子的正下方与对角线方向格子的组合符合语法规则，则此格子也符合语法规则，最后检查根节点是否符合语法规则。
> ![[CKY.png|350]]
> 由于一个句子可能存在多种语法树，概率化之后，每个格子中可记录一个概率，如此可以得到概率最大的语法树。

### 依存式语法
#### 基本概念
> [!definition] 依存式语法
> 依存式语法是通过各个词之间的依存关系来构建语法树的语法，适用于那些语法成分间顺序不重要的语言。
> ![[dependency_grammar.png|293]]

#### 分析方法
使用[[程序语言与编译原理#^03eb9d|移入-规约算法]]进行分析。

### 基于神经网络的语法解析
> [!definition] 词向量
> 词向量通常是一个300维的向量，其空间上的远近关系能够在一定程度上反映词语之间的语义关系。

> [!definition] 分层Softmax (Hierarchical Softmax)
> 利用Huffman编码的思想，将输出层的$N$分类（$N$为词汇表大小）转化为$N-1$个二分类，词频越高的词，其二分类的路径相对越短，大大减少了计算量。

> [!definition] 负采样 (Negative Sample)
> 同样是为了减少庞大的词汇表带来的巨大计算量。每次迭代只计算出很小一部分词的概率，然后最小化这些词的概率。这些词通过噪声分布随机选出，噪声分布为$P(w_i)=\frac{\alpha_i^{3/4}}{\sum_j\alpha_j^{3/4}}$，其中$\alpha_i$为词频。

## 具体任务
### 情感分类
> [!definition] 情感分类
> 情感分类是指输入一段短文本，输出情感类别标签的过程。简单的极性一般分为正面、负面、中性三类，而更加具体的分类还能分出更多情感以及强度。

### 文本匹配
> [!definition] 文本匹配
> 文本匹配的输入是两段文本，输出为两段文本的匹配程度。

文本匹配的应用：
- **问答系统**：将用户的问题与数据库中的标准问题进行匹配，从而找出答案；
- **阅读理解**：将选项与问题和文章进行匹配，从而找出答案；
- **信息检索**：将用户的查询与数据库中的文档进行匹配，从而找出最相关的文档。

文本匹配的实现方法：将两段文本投入编码器，比较这两段文本编码的匹配程度。
