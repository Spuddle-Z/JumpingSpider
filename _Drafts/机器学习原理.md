\subsubsection{代价函数 Cost Function}

\begin{formal}

    代价函数反映的是模型预测的数据与实际数据间的偏差，也即代价函数越小，机器的预测结果越接近实际。

\end{formal}

每个机器学习模型都有可学习的参数$\theta$，因此\textbf{机器学习程序的最终目的是求出使代价函数$J(\theta)$最小的$\theta$值}，即求

$$\arg\min_{\theta}\ J(\theta)$$

\begin{example}

    \begin{itemize}

        \item \textbf{平方误差函数}通常用于一些简单的回归问题，如线性回归。其函数如下：

        $$J(\theta)=\frac{1}{2m}\sum^m_{i=1}\left(h_\theta(x)-y\right)^2$$

        \item 对于二分类问题如Logistic Regression，则使用以下函数：

        $$J(\theta)=-\frac{1}{m}\sum^m_{i=1}\left[y\log h_\theta(x)+(1-y)\log(1-h_\theta(x))\right]$$

        \item Softmax Regression与Logistic Regression同为分类问题，但其输出$h_\theta(x)$为一$k$维向量，$h_\theta(x)_i$表示结果是第$i$个选项的可能性。由于所有概率非负且和为1，所以原始结果$o$需要进一步处理，也即引入softmax函数：

        $$

        \begin{aligned}

            h_\theta(x)&=\mathrm{softmax}(o) \\

            &\Updownarrow \\

            h_\theta(x)_i&=\frac{\exp(o_i)}{\sum_k\exp(o_k)}

        \end{aligned}

        $$

        其损失函数为交叉熵，且由于$y$只有某一维是1，其余维都是0，所以损失函数可作如下简化：

        $$H(y,h_\theta(x))=-\sum_iy_i\log h_\theta(x)_i=-\log y^Th_\theta(x)$$

        \item 对于生成序列的好坏，一般用\textbf{BLEU(Bilingual Evaluation Understudy)}来衡量，此值越高，生成序列越好。BLEU的区间为$[0,1]$，可以由$S_1,S_2$两部分相乘得到，$S_1$用于惩罚过短的预测，$S_2$用于奖励更长的匹配：

        $$

        % ***

        \left\{\begin{aligned}

            BLUE&=S_1\cdot S_2 \\

            S_1&=\exp\left(\min\left\{0,1-\frac{len_{label}}{len_{pred}}\right\}\right) \\

            S_2&=\prod_{i=1}^np_i^{2^{-i}}

        \end{aligned}\right.

        $$

        其中$p_n$为n-gram的预测精度：

        \begin{example}

            例如对于序列Today is a nice day，预测结果为It is a nice day today，则此时$p_1=\frac{5}{6},p_2=\frac{3}{5},p_3=\frac{2}{4},p_4=\frac{1}{3},p_5=0$

        \end{example}

    \end{itemize}

\end{example}

## Gradient Descent 梯度下降法
$$\theta_{n+1}=\theta_n-\alpha\frac{\partial}{\partial\theta}J(\theta_n)$$
其中，$\alpha$为学习率，控制步长大小；$\theta_n$为算法循环n次后所得参数向量。
## 特征工程
- **特征工程(Feature Engineering)**：即用对问题的知识和直觉来从已有的原始特征中构造新的特征，来使机器更容易学习。

> [!example] 
> 在预测一块地皮的价格时，我们可以用原有的特征——长与宽，来组合成与地皮价格更加相关的特征——面积，从而提高模型预测的准确率。
