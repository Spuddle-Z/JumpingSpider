  

\subsection{提高深度学习系统性能}

\subsubsection{深度学习网络基本结构}

\begin{formal}

    \begin{itemize}

        \item \textbf{Backbone}：主干网络，用来提取特征，一般使用resnet、VGG等固定结构；

        \item \textbf{Neck}：融合从backbone不同层提取的特征，进一步提取特征；

        \item \textbf{Head}：利用所提特征来做出预测；

        \item \textbf{Loss}：损失函数，用来衡量预测结果与真实结果的差异。

    \end{itemize}

\end{formal}

\subsubsection{其它常见结构}

\begin{formal}

    \begin{itemize}

        \item \textbf{Bottleneck}：使用$1\times1$的卷积核降低通道数，大幅减少计算量；

        \item \textbf{GAP (Global Average Pooling)}：全局平均池化，将特征图的每个通道的特征值取平均，得到一个特征值，最后将所有通道的特征值拼接起来，作为最终的特征向量；

            \begin{center}

                \includegraphics[width=.8\textwidth]{figure/GAP.jpg}

            \end{center}

    \end{itemize}

\end{formal}

\subsubsection{丢弃法}

\begin{formal}

    \textbf{丢弃法}(Dropout)即对隐藏层的输入$x$做如下处理后，将$x'$输入下一层。

    $$

    x_i'=

    \left\{\begin{aligned}

        0 &,\mathrm{with\ probablity}\ p \\

        \frac{x_i}{1-p}&,\mathrm{otherwise}

    \end{aligned}\right.

    $$

\end{formal}

此操作前后$x$期望不变，相当于将隐藏层中的部分神经元丢弃，而去训练一个子网络，最后的模型即是将多个这样训练出来的子网络取平均。之后证明这种方法与正则化相似，但其实际效果要强于正则化。

\begin{warning}

    神经网络越复杂，丢弃法的影响越大。因此可以将神经网络结构设得复杂一点，再通过丢弃法控制模型复杂度，效果会更好。

\end{warning}

\subsubsection{梯度爆炸与梯度消失}

对于一$d$层的神经网络$h_t=f_t(h_{t-1})$那么损失$J$关于第$t$层参数$W_t$的梯度计算则涉及到一连串矩阵的乘法：

$$\frac{\partial J}{\partial W_t}=\frac{\partial J}{\partial h_d}\frac{\partial h_d}{\partial h_{d-1}}...\frac{\partial h_{t+1}}{\partial h_t}\frac{\partial h_t}{\partial W_t}$$

\begin{formal}

    此处若这些矩阵的元素大于1，积累到一定次数，梯度将超出值域，这种现象称为\textbf{梯度爆炸}；同理，若矩阵元素小于1，积累到一定次数，梯度将归于0，这种现象称为\textbf{梯度消失}。

\end{formal}

梯度爆炸的问题：对学习率敏感，学习率太大则加速爆炸，太小则训练无进展。\\

梯度消失的问题：学习无进展，尤其是底部层，因此无法使神经网络更深。

以下为其解决方法：

\begin{itemize}

    \item 选择合适的初始化 \\

    令$h_{t-1}$独立于$W_t$，且设$E(W_t)=0$，$D(W_t)=\gamma_t$为常数。可求出$h_t$的正向方差为$D(h_t)=n_{t-1}\gamma_tD(h_{t-1})$；与正向情况类似，可求出$D\left(\frac{\partial J}{\partial h_{t-1}}\right)=n_t\gamma_tD\left(\frac{\partial J}{\partial h_t}\right)$。要保持方差不变，则需$n_{t-1}\gamma_t=n_t\gamma_t=1$，但此条件难以满足，运用Xavier初始使$\gamma_t=\frac{2}{n_{t-1}+n_t}$。所以初始权重可根据正态分布$\mathcal{N}\left(0,\sqrt{\frac{2}{n_{t-1}+n_t}}\right)$生成。

    \item 选择合适的激活函数 \\

    要保持均值与方差不变，则激活函数需要满足其在零点处的切线斜率为1，截距为0。由此，激活函数可选$\tanh(x),\mathrm{ReLU}(x),4\mathrm{sigmoid}(x)-2$等。

    \begin{center}

        \includegraphics[width=0.4\textwidth]{figure/sigma.png}

    \end{center}

    \item 梯度裁剪 \\

    若梯度的模长大于$\theta$，则通过投影将其模长拖回$\theta$。此方法可有效地预防梯度爆炸。

\end{itemize}

\subsubsection{批量处理}

批量处理数据可以允许模型的并行计算，从而加快训练速度，但批量大小的选择也会影响模型的性能，因此批量大小也会作为一个超参数来选择。通常来说，批量越大：

\begin{itemize}

    \item 每次迭代所需的时间越短

    \item 每次迭代损失减少得越慢

    \item 模型的泛化能力越差

\end{itemize}

总体来说，批量大小越大，模型的收敛速度越慢，且模型的泛化能力越差。这主要是因为小批量所找到的最小值更加平坦，且大批量找到的最小值会离初始权重较近。然而，可以用增大学习率的方式来增加收敛速度，但其性能依然不及小批量。

\subsubsection{批量归一化 Batch Normalization}

在简单的机器学习中，为防止收敛速度过慢，输入一般都是需要归一化的。但对于神经网络来说，其每层都相当于一个简单的机器学习模型，若只有第一层输入归一化，之后每层输入的分布将会失去控制，导致收敛变慢。\\

但若仅仅在每层添加归一化步骤，当神经网络很深时，由于损失的计算在最后，反向传播时顶部的层训练很快，底部的层则训练较慢，但底层一变化，顶层就得根据新的顶层重复学习，收敛速度同样不高。因此我们需要添加额外的参数，及时调整每层数据的分布。

\begin{formal}

    \textbf{批量归一化}即取一批数据在第$i$层的输出$z_i$均值$\mu_i$与方差$\sigma_i^2$，归一化后得出$\tilde{z}_i$：

    $$\tilde{z}_i=\frac{z_i-\mu_i}{\sigma_i}$$

    添加额外的参数$\beta_i,\gamma_i$，得到最终的输出$\hat{z}_i$：

    $$\hat{z}_i=\gamma\tilde{z}_i+\beta$$

\end{formal}

此方法允许使用更高的学习率，从而加速模型收敛，但不会改变模型精度。

\begin{warning}

    有论文指出批量归一化就是通过在每个小批量里加入噪音来控制模型复杂度，因此不必与丢弃法混用。

\end{warning}

\subsubsection{数据增强}

\begin{formal}

    通过变形数据来获取多样性，从而泛化模型，使其鲁棒性更好。同样是一个防止过拟合的方法。

\end{formal}

\subsubsection{迁移学习 Transfer Learning}

一个神经网络一般可以分为两个部分，底层部分是用来提取特征的，顶层的全连接部分是用来总结分析特征来分类模型的。不同模型可能需要分类的任务不一样，但提取的特征是可以重复使用的。

\begin{formal}

    \textbf{迁移学习}即是用大数据训练好的复杂模型的参数对某个特定任务模型来进行初始化，来简化特定任务模型的训练，并提高此模型的准确性。其中，用来初始化的模型称为\textbf{预训练模型}（Pre-train），训练预训练模型的大数据集称为\textbf{源数据集}，用小规模数据继续在预训练模型上训练的操作称为\textbf{微调}（Fine Tuning）。

\end{formal}

\begin{warning}

    源数据集要远复杂于目标数据，所以为避免模型在目标数据集上过拟合，一般会使用更强的正则化，通常通过以下手段实现：

    \begin{itemize}

        \item 使用更小的学习率；

        \item 迭代更少的次数；

        \item 由于低层次的特征更加通用，有时甚至可以固定住底层的参数不参与训练。

    \end{itemize}

\end{warning}

迁移学习省去了模型训练冷启动的时间，通常速度更快，精度更高。