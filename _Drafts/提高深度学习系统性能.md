  

\subsection{提高深度学习系统性能}

\subsubsection{深度学习网络基本结构}

\begin{formal}

    \begin{itemize}

        \item \textbf{Backbone}：主干网络，用来提取特征，一般使用resnet、VGG等固定结构；

        \item \textbf{Neck}：融合从backbone不同层提取的特征，进一步提取特征；

        \item \textbf{Head}：利用所提特征来做出预测；

        \item \textbf{Loss}：损失函数，用来衡量预测结果与真实结果的差异。

    \end{itemize}

\end{formal}

\subsubsection{其它常见结构}

\begin{formal}

    \begin{itemize}

        \item \textbf{Bottleneck}：使用$1\times1$的卷积核降低通道数，大幅减少计算量；

        \item \textbf{GAP (Global Average Pooling)}：全局平均池化，将特征图的每个通道的特征值取平均，得到一个特征值，最后将所有通道的特征值拼接起来，作为最终的特征向量；

            \begin{center}

                \includegraphics[width=.8\textwidth]{figure/GAP.jpg}

            \end{center}

    \end{itemize}

\end{formal}

\subsubsection{丢弃法}

\begin{formal}

    \textbf{丢弃法}(Dropout)即对隐藏层的输入$x$做如下处理后，将$x'$输入下一层。

    $$

    x_i'=

    \left\{\begin{aligned}

        0 &,\mathrm{with\ probablity}\ p \\

        \frac{x_i}{1-p}&,\mathrm{otherwise}

    \end{aligned}\right.

    $$

\end{formal}

此操作前后$x$期望不变，相当于将隐藏层中的部分神经元丢弃，而去训练一个子网络，最后的模型即是将多个这样训练出来的子网络取平均。之后证明这种方法与正则化相似，但其实际效果要强于正则化。

\begin{warning}

    神经网络越复杂，丢弃法的影响越大。因此可以将神经网络结构设得复杂一点，再通过丢弃法控制模型复杂度，效果会更好。

\end{warning}


\subsubsection{批量处理}

批量处理数据可以允许模型的并行计算，从而加快训练速度，但批量大小的选择也会影响模型的性能，因此批量大小也会作为一个超参数来选择。通常来说，批量越大：

\begin{itemize}

    \item 每次迭代所需的时间越短

    \item 每次迭代损失减少得越慢

    \item 模型的泛化能力越差

\end{itemize}

总体来说，批量大小越大，模型的收敛速度越慢，且模型的泛化能力越差。这主要是因为小批量所找到的最小值更加平坦，且大批量找到的最小值会离初始权重较近。然而，可以用增大学习率的方式来增加收敛速度，但其性能依然不及小批量。

\subsubsection{数据增强}

\begin{formal}

    通过变形数据来获取多样性，从而泛化模型，使其鲁棒性更好。同样是一个防止过拟合的方法。

\end{formal}

\subsubsection{迁移学习 Transfer Learning}

一个神经网络一般可以分为两个部分，底层部分是用来提取特征的，顶层的全连接部分是用来总结分析特征来分类模型的。不同模型可能需要分类的任务不一样，但提取的特征是可以重复使用的。

\begin{formal}

    \textbf{迁移学习}即是用大数据训练好的复杂模型的参数对某个特定任务模型来进行初始化，来简化特定任务模型的训练，并提高此模型的准确性。其中，用来初始化的模型称为\textbf{预训练模型}（Pre-train），训练预训练模型的大数据集称为\textbf{源数据集}，用小规模数据继续在预训练模型上训练的操作称为\textbf{微调}（Fine Tuning）。

\end{formal}

\begin{warning}

    源数据集要远复杂于目标数据，所以为避免模型在目标数据集上过拟合，一般会使用更强的正则化，通常通过以下手段实现：

    \begin{itemize}

        \item 使用更小的学习率；

        \item 迭代更少的次数；

        \item 由于低层次的特征更加通用，有时甚至可以固定住底层的参数不参与训练。

    \end{itemize}

\end{warning}

迁移学习省去了模型训练冷启动的时间，通常速度更快，精度更高。