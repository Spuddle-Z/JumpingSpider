---
tags:
  - Knowledge
---
## 背景
> [!definition] 维数灾难 (Curse of Dimensionality)
> 高维空间中会出现许多反直觉的现象，具体来说，样本点在高维空间中会变得极其稀疏，与此同时，欧式距离也将失效。

在高维空间中，边长为1的超立方体的内切超球体的体积公式如下：
$$V(d)=\frac{\pi^{\frac{n}{2}}}{2^{d}\cdot\Gamma\left( \frac{n}{2}+1 \right)}$$
可以推得
$$\lim_{d\to\infty}V(d)=0$$
这意味着在高维空间中，样本点几乎全部分布在超立方体的边缘。

我们若希望直观地了解高维空间中点的分布，就需要一个降维的方法。此时我们需要保留的是样本点之间关系的信息，而样本的具体分布则可以忽略。如下图，对于在S形带上分布的一系列样本点，我们不需要还原其S形的分布，只需将其展开，铺在二维平面上。
![[Pasted image 20240514170408.png|500]]

因此，我们的目标是将高维的数据投影到二至三维的空间内，而尽可能不改变其近邻关系。
## t-SNE
我们定义原样本点为$x_i$，变换后的样本点为$h_i$。
> [!definition] t-分布随机近邻嵌入 (t-distributed Stochastic Neighbor Embedding, t-SNE)
> 高维空间中，$P_{ij}=P(x_j|x_i)$表示点$j$和点$i$有近邻关系的概率
> $$
> \begin{aligned}
> P_{ij}&=\frac{\exp\left( \frac{-|x_j-x_i|^2}{2\sigma_i^2} \right)}{\sum_{k=1}^{n}\exp\left( \frac{-|x_k-x_i|^2}{2\sigma_i^2} \right)}\\
> &\propto \exp\left( \frac{-|x_j-x_i|^2}{2\sigma_i^2} \right)\\
> &\propto N(x_i,\sigma_i^2)
> \end{aligned}
> $$
> 而低维空间中，$Q_{ij}=P(h_j|h_i)$表示变换后的$j$与$i$有近邻关系的概率
> $$
> \begin{aligned}
> Q_{ij}&=t(h_i)\\
> &=\frac{(1+|h_j-h_i|^2)^{-1}}{\sum_{k=1}^{n}(1+|h_k-h_i|^2)^{-1}}
> \end{aligned}
> $$
> 此处并未使用正态分布，而是使用t分布。
> 
> 我们需要使$P,Q$的分布尽量相等，因此定义目标函数为
> $$\min_{h_i}KL(P||Q)$$
> 可以看到$h$是优化得到的，而并非是计算求得的。

> [!caution] 
> t-SNE只能保证邻近的点在变换之后依然邻近，但不能保证本来相距较远的点变换前后的距离能够成比例变化。
## LLE
**符号表示**：
- $x_i$代表第$i$个原数据，$h_i$代表第$x_i$变换后得到的嵌入向量；
- $N_i$代表第$i$个样本点的近邻节点的集合。

> [!definition] 局部线性嵌入 (Locally Linear Embedding, LLE)
> 利用近邻节点的线性组合来表示节点，即
> $$x_i\approx\sum_{j\in N_i}w_{ij}x_j$$
> 由此，我们可以利用$W$来表示节点与其附近节点的关系。我们通过优化下面的目标函数来得到$W$
> $$\min_{w_{ij}}\left\lvert x_i-\sum_{j\in N_i}w_{ij}x_j\right\rvert^2$$
> 得到节点关系$W$之后，再通过优化
> $$\min_{h_i}\sum_i\left\lvert h_i-\sum_{j\in N_i}w_{ij}h_j\right\rvert^2$$
> 得到符合此关系的嵌入节点$h_i$。

