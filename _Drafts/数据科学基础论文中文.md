### 任务简述

GNN 在异质图和同构图的节点分类和链路预测任务中表现非常好。在很多推荐场景中，利用用户之间的信任关系和产品的本质属性，结合用户的购买记录，使用 GNN 生成的表征也能达到很好的效果。同时，在学术平台中，合作者推荐、论文推荐、期刊和会议的审稿人推荐是主要任务，GNN 在这些任务上的表现依然良好。对于上述问题，都有相对较好的基准指标和模型。但是，在论文推荐的场景中，如果是通过合作者和论文所在的领域和合作社群进行科学研究的学者，引文推荐仍然缺乏更好的数据集和模型。

本选题提供的数据集用在一个学术推荐系统中，其中“用户”为学术论文的作者，“产品”为学者的论著。当用户和产品都有关联网络时，我们可以提取用户群体和产品类别的群体特征，基于产品之间的关系和用户之间的联系，为用户提供更好、更多样的推荐，这可能有助于解决推荐系统冷启动造成的用户行为较少的问题。

由此，本模型解决了一个学术网络中的推荐问题。我们用从地理科学领域的顶级期刊中收集的6611 位作者和相应的 79937 篇论文，以及他们出版物的引文信息为基础数据，构建了一个异质学术网络。此网络中包含两类节点，一类节点代表作者，另一类代表论文。在这个网络中，作者节点和论文节点之间的每条边都表示作者阅读过该论文（连接作者和被作者所写的论文所引用的论文），两个作者节点之间的每条边表示合著关系，两个论文节点之间的每条有向边则表示引用关系。我们的任务是根据所提供的信息预测测试集中的每个作者-论文对。如果该论文会被推荐给作者，则标记为 1，否则标记为 0。

### 相关工作

最近有很多研究对基于 GNN 的推荐系统进行了研究，其总体框架如下，主要包括从用户和项目之间的交互构建图，使用调整后的聚合方法通过 GNN 块对节点特征表示进行编码，最后根据特征进行链接预测。下面总结了针对此问题的五种具体方法。

图卷积矩阵补全 (GC‑MC) ：GC‑MC 利用用户‑项目交互的图结构来增强协同过滤过程。它采用图卷积运算来捕获用户和项目之间的高阶关系，从而实现信息在图中传播的效果。通过结合用户‑项目交互和图结构， GC‑MC 可以学习更准确的表示，并对用户‑项目交互矩阵中的缺失条目做出更好的预测。该模型利用低秩矩阵分解和图卷积层的组合来联合优化用户和项目的潜在嵌入。这种方法使 GC‑MC 能够有效地捕获协同过滤信号以及用户和项目之间基于图的关系。

PinSage：作者提出了一个大规模深度推荐引擎，在 Pinterest 开发了一种数据高效的图卷积网络 (GCN) 算法，名为PinSage，该算法结合了高效的随机游走和图卷积来生成节点的嵌入。PinSage 使用局部卷积模块为节点生成嵌入，从输入节点特征开始，然后学习神经网络，这些神经网络在图上转换和聚合特征以计算节点嵌入。PinSage 的算法与GraphSage 非常相似，略有不同，论文分为两个部分讲解：卷积算法和小批量。小批量的过程与 GraphSage 中小批量算法的过程相同。以下是卷积算法。

DiffNet ：DiffNet 提出了一种可微分神经架构搜索方法。它是一个自动化设计神经网络架构过程的框架。DiffNet 采用基于梯度的优化技术，通过联合学习架构参数和网络权重来搜索最佳架构。这种方法允许发现针对特定任务（例如图像分类或自然语言处理）定制的神经网络架构，而无需手动设计或专业知识。

Light GCN：LightGCN 是一种图卷积网络模型，专注于基于协同过滤的推荐系统。它通过删除特定于用户和特定于项目的嵌入层来简化传统的图卷积网络架构。LightGCN 仅依靠用户‑项目交互矩阵通过分层传播方案传播信息，从而实现高效的计算和可扩展性。通过直接使用用户‑项目交互数据， LightGCN 可以有效地捕获图结构中的协同过滤信号。

**异构图注意力网络 (HAN)：** HAN 是专为异构图设计的注意力机制，有效地处理了图中的节点多样性和边的多样性问题。该模型引入了两级注意力机制：节点级别和语义级别。节点级别的注意力利用图注意力网络（GAT）沿元路径聚合邻居节点的信息，能够优化邻居节点信息的权重分配。语义级别的注意力则通过多个元路径整合，动态地学习并加权不同元路径下的节点表示，从而更好地捕捉节点间复杂的关系和语义信息。这种两层结构使得HAN在节点分类和链路预测等任务上表现出色。

我们一开始使用的就是HAN模型，但由于某些原因，其效果并不是非常理想。我们猜测有以下原因：
HAN的计算成本较高，因为需要计算节点级和语义级的注意力分数。
模型过度依赖于图结构信息，可能会忽视节点和边本身的属性信息，这种偏向在某些应用场景中可能影响到模型的表现效果。




对于初始嵌入来说，我们只是简单地利用meth2vec的方法进行了作者节点的嵌入，并未在此方面做太多研究工作。因此，增强初始节点嵌入是可以探索的另一个方面。通过结合特定领域的知识或利用更先进的技术来生成节点嵌入，我们理论上可以为模型提供更丰富、更具信息量的节点表示，从而使模型做出更准确的预测。

除去对节点的嵌入，另一种可能的增强方法是考虑将边缘特征也纳入模型架构。边缘特征可以提供有关节点之间关系的宝贵信息，并可能增强模型捕获复杂模式和依赖关系的代表性能力。

探索性能更好的Backbone是未来研究的另一个有希望的方向。虽然我们的架构表现已经比较出色，但研究更多最先进的层骨干，如之前提到的HGT、LightGCN、 DiffNet 和 GC‑MC，可能会进一步提高模型的精度和鲁棒性。

更好的技巧还可以研究其他技术，例如残差连接和 dropout，以增强模型的泛化能力并减轻过度拟合。此外，探索将不同的 GNN 架构与其他算法相结合的混合方法或结合深度行走和 node2vec 等拓扑特征可以为链接预测任务带来更全面、更强大的模型。

#### 模型细节

我们发现给出的baseline中并没有激活函数，也就是说原模型是一个线性的变换，这样的结构难以拟合出复杂的模型。因此，我们在层与层之间都加入了激活函数，来为模型引入非线性。

最开始我们也是先使用了最为常见的ReLU函数来充当激活函数，在调试模型的过程中，我们又测试了其他激活函数，如ELU与LeakyReLU函数。我们发现虽然ELU和LeakyReLU函数间的效果并没有什么区别，但二者相对于ReLU函数，性能却有一个明显的提升。鉴于ELU与LeakyReLU在负半轴的数值均是非零的，由此我们有理由猜测我们的网络中出现了神经元死亡的问题，而由于这两个函数在负半轴仍有数值，恰好缓解了这个问题。我们最终选择了ELU函数作为神经网络层间的非线性函数。

在学习率的选择上，我们使用了余弦退火调度器。其对学习率的更新如下：
$$lr(t)=lr_{\min}+\frac{1}{2}(lr_{\max}-lr_{\min})\left( 1+\cos\left( \frac{t}{T_{\max}}\pi \right) \right)$$
其中
- $lr(t)$是第$t$个周期的学习率；
- $lr_{\min}$和$lr_{\max}$分别是学习率的下界和上界，下界一般接近于0，而上界通常设置为初始学习率；
- $t$是当前迭代次数；
- $T_{\max}$是每次学习率周期的迭代次数。

学习率在一个周期内的变化如图：

由于模型参数在初始化时是非常不稳定的，因此在刚开始时需要选用小一点的学习率，防止一开始向某个错误的方向大踏步前进。等到模型迭代了几轮大概找到了优化方向时，再将学习率快速增大。开始时这个将学习率由小变大的过程称为热身（warmup）。

热身完成后，便与其他调整学习率的思路一样，开始令学习率慢慢下降。但是区别于平常学习率的指数下降，余弦退火原理中的学习率是像余弦曲线一样，先缓慢下降，维持一段时间较高的学习率，这样一定程度上能够加快模型的收敛。而后再快速下降，最后缓慢下降到最低值。

上述过程只是余弦退火算法的一个周期。当迭代数$t$达到$T_{\max}$时，此算法将会重置学习率，使得学习率有机会再一次增大，这个操作能够帮助模型跳出局部最小值或是渡过较长的平台期。经测试，此算法确实大大地加速了模型的收敛，并使模型达到了之前从未达到的最小值。