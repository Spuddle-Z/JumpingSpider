---
tags:
  - Knowledge
aliases:
  - Reinforcement Learning
  - RL
---
## Alpha Go
### 基本概念
![[Pasted image 20240522184031.png|375]]
其由**策略网络(Policy Network)**与**价值网络(Value Network)**两个网络共同组成。
- 策略网络输出的是下一步棋该走什么地方的概率$P_{\sigma}(a|S)=softmax(h(S))$，其中$S$是当前棋盘状态，$a$是下一步行动，；
- 价值网络的输出是下在某处的潜在价值$P_{\theta}(S)$。

### 训练方法
1. **行为克隆(Behavior Cloning)**：以人类棋手的对弈数据为标签，进行监督训练
	$$\Delta\sigma\propto\frac{\partial }{\partial \sigma}\log P_{\sigma}(a^*|S)$$
1. **策略梯度(Policy Gradient)**：人类的对弈数据并不多，当通过行为克隆训练出一个较为粗糙的模型后，用其参数$\sigma$初始化新模型参数$\rho$，并用新模型与之对弈。如果输了，则$z=1$；如果赢了，则$z=-1$。
	$$\Delta\rho\propto\frac{\partial }{\partial \rho}\log P_{\rho}(a|S)\cdot z$$
	其本质上在提升
	$$E_{\rho}[z]=\sum_iP(z=1|a_1,a_2,\cdots,a_{n},S_0,S_1,\cdots,S_{n})$$
