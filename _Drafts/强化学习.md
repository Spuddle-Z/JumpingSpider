---
tags:
  - Knowledge
aliases:
  - Reinforcement Learning
  - RL
---
## Alpha Go
### 基本概念
![[Pasted image 20240522184031.png|375]]
其由**策略网络(Policy Network)**与**价值网络(Value Network)**两个网络共同组成。
- 策略网络输出的是下一步棋该走什么地方的概率$P_{\sigma}(a|s)=softmax(h(s))$，其中$s$是当前棋盘状态，$a$是下一步行动，；
- 价值网络的输出是下在某处的潜在价值$v_{\theta}(s)$。

### 训练策略网络
1. **行为克隆(Behavior Cloning)**：以人类棋手的对弈数据为标签，进行监督训练
	$$\Delta\sigma\propto\frac{\partial }{\partial \sigma}\log P_{\sigma}(a^*|s)$$
1. **策略梯度(Policy Gradient)**：人类的对弈数据并不多，当通过行为克隆训练出一个较为粗糙的模型后，用其参数$\sigma$初始化新模型参数$\rho$，并用新模型与之对弈。如果输了，则$z=1$；如果赢了，则$z=-1$。
	$$\Delta\rho\propto\frac{\partial }{\partial \rho}\log P_{\rho}(a|s)\cdot z$$
	其本质上在提升
	$$E_{\rho}[z]=\sum_{a,s}P(z=1|a_1,\cdots,a_{n},s_1,\cdots,s_{n+1})\cdot P(a_1,\cdots,a_{n},s_2,\cdots,s_{n+1}|s_1)$$
	其中
	$$P_{\rho}(a_1,...,a_n,s_2,...,s_{n+1}|s_1)=\prod_{i=1}^{n}P(s_{i+1}|a_i,s_i)P_{\rho}(a_i|s_i)$$
	$P(s_{i+1}|a_i,s_i)$是对手的下法；$P_{\rho}(a_i|s_i)$是我们的策略，是策略网络可以学习的分布。我们称这个架构为**动态模型(Dynamic Model)**。

> [!note] 
> 为避免每次下棋都采用同样的下法，对于行动的选择不是每次都选择获胜概率最大的下法，而是根据获胜概率随机采样，胜率大的下法被选中的概率就高。
### 训练价值网络
$$\Delta\theta\propto\frac{\partial }{\partial \theta}[z-v_{\theta}(s)]^2$$
其中$z$表示经过状态$s$的所有情况的平均胜率。



0517-2-1634 #Missing 