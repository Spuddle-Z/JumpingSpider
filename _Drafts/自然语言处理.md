---
tags:
  - Knowledge
aliases:
  - Natural Language Processing
---
## NLP基础

\subsection{TF-IDF}

\begin{formal}

    \textbf{倒排}是指提前遍历所有文档，存储出现的关键词，搜索时直接根据关键词进行搜索。其复杂度较遍历所有文档大大降低。

\end{formal}

\begin{formal}

    \textbf{词频}（Term Frequency）表示某个关键词在文档中出现的频率，其计算公式如下：

    $$tf_{i,j}=\frac{n_{i,j}}{\sum_kn_{k,j}}$$

    其中$n_{i,j}$表示关键词$w_i$在文档$d_j$中出现的次数。

\end{formal}

\begin{formal}

    \textbf{逆向文档频率}（Inverse Document Frequency）则负责降低常用词的权重，如“的”、“是”等。其计算公式如下：

    $$idf_i=\log\frac{|D|}{|\{j:w_i\in d_j\}|}$$

    其中$|D|$表示文档总数，$|\{j:w_i\in d_j\}|$表示包含关键词$w_i$的文档数。

\end{formal}

\begin{formal}

    \textbf{TF-IDF}是用于信息检索与搜索引擎上的一种统计模型，通过公式$tf-idf_{i,j}=tf_{i,j}\times idf_i$来得到关键词$w_i$在文档$d_j$中的权重，此值越大匹配越好。

\end{formal}

  

\subsection{评价语言模型性能}

给出一段\textbf{足够长的、真实的、模型没有见过}的语料，则模型对这段语料预测的概率越高，则说明这个模型越好。

\begin{itemize}

    \item 足够长——避免模型“运气好”，刚好在要预测的几个位置上表现良好；

    \item 真实——只有在真实语料上模型预测的概率才有意义；

    \item 模型没有见过——避免模型背下来整个训练集，然后测试的时候直接默写。

\end{itemize}

\begin{formal}

    \textbf{混淆度}（Perplexity）指在测试集中，平均每个模型做出预测的位置上，那个

    实际出现的真实词被模型赋予的概率的倒数。对于一个由$N$个单词组成的语料，模型给出此测试集的概率为$\prod_{i=1}^NP(w_i|w_1,\cdots,w_{i-1})$，接着对测试集长度归一化（因为是相乘，所以用几何平均）得到$\sqrt[N]{\prod_{i=1}^NP(w_i|w_1,\cdots,w_{i-1})}$，在此基础上取倒数得到下式：

    $$PPW=\frac{1}{\sqrt[N]{\prod_{i=1}^NP(w_i|w_1,\cdots,w_{i-1})}}$$

\end{formal}

对于完全没受过训练的语言模型，其$PPW=|V|$，其中$|V|$表示词汇表的大小；而对于一个完美的语言模型，其$PPW=1$。因此，我们希望模型的$PPW$越小越好。

  

\subsection{词性标注}

\begin{formal}

    \textbf{词性标注}就是用算法自动将句子中的每个词标注其语法属性的过程。

\end{formal}

\begin{formal}

    我们运用\textbf{隐马尔可夫模型}（Hidden Markov Model, HMM）来进行词性标注。其模型示意图与公式如下：

    \begin{center}

        \includegraphics[width=0.5\textwidth]{figure/HMM.png}

    \end{center}

    $$

    \left\{\begin{aligned}

        \mathop{\arg\max}\limits_hP(h|v)&=\mathop{\arg\max}\limits_h\frac{P(v|h)\cdot P(h)}{P(v)}=\mathop{\arg\max}\limits_hP(v|h)\cdot P(h)\\

        P(h)&=P(h_1)\prod_{i=2}^NP(h_i|h_{i-1})\\

        P(v|h)&=\prod_{i=1}^NP(v_i|h_i)

    \end{aligned}\right.

    $$

    其中$v$代表一句话中的词，$h$代表其对应的词性。HMM的目标是给定一句话$v$，得到最可能的词性$h$，即求得$\mathop{\arg\max}\limits_hP(h|v)$。\\

    一般用对数将连乘转化为连加。

\end{formal}

直接使用隐马尔可夫模型需要计算所有的$P(h,v)$，其计算量过于庞大，因此引入\textbf{维特比算法}（Viterbi Algorithm）来简化模型。

\begin{formal}

    Viterbi算法的思想是，顺序计算每个词词性的最大概率，然后在此基础上计算下一个词词性的最大概率，直到计算完所有词的词性。\\

    由于此方法可能遗漏最优解，因此可以使用束搜索对其进行优化。

\end{formal}

\begin{formal}

    前向-后向算法(Forward-Backward Algorithm)是一种无监督学习算法。

\end{formal}

  

\subsection{中文分词}

\subsubsection{方法概览}

中文分词按照是否使用词典可以分为\textbf{有词典切分}和\textbf{无词典切分}。有词典切分中基于规则的方法（如最大匹配法与最短路径法）比较机械，无法避免一些较为灵活的歧义；而与之相对的基于统计的方法则可以避免这些问题，但是需要大量的语料进行训练。

\begin{formal}

    \textbf{语言模型法}是一种有词典切分的基于统计的方法。其思想是从头开始预测之后一段文字的数种切分方式，然后引入束搜索的思想来优化模型的计算规模。

\end{formal}

\subsubsection{条件随机场 Conditional Random Field (CRF)}

\begin{formal}

    \textbf{条件随机场}是将分词问题看作一个词性标注问题的分词方法。具体做法是给每个汉字标注词首(B)、词中(M)、词尾(E)、单字成词(S)四种词性中的一种，而后根据标注来划分单词。

    \begin{example}

        以下图为例：

        \begin{center}

            \includegraphics[width=0.5\textwidth]{figure/CRF_example.png}

        \end{center}

    \end{example}

    CRP标注词性的模型如下：

    \begin{center}

        \includegraphics[width=.5\textwidth]{figure/CRF_structure.png}

    \end{center}

    $$

    \left\{\begin{aligned}

        \mathop{\arg\max}\limits_hP(h|v)&=\mathop{\arg\max}\limits_h\frac{score(h,v)}{\sum_hscore(h,v)}\\

        score(h,v)&=\exp\left(\sum_{k=1}^{K}w_kF_k(h,v)\right)\\

        F_k(h,v)&=\sum_{i=2}^{N}f_k(h_{i-1},h_i,v,i)

    \end{aligned}\right.

    $$

    其中$f_k(h_{i-1},h_i,v,i)$一般是值域为$\{0,1\}$的二值函数，但也可以非常灵活，因此其允许我们在某个字预测非常不好时，手动定义此特征函数。

    \begin{unimportant}

        对于一个实际的CRF分词器来说，其$f_k$的数量会在$10^5$\~{}$10^6$量级，因此需要算法按照特征模板(Feature Template)来生成$f_k$。

    \end{unimportant}

\end{formal}

CRP的训练是一个监督学习，我们的目标是求出

$$\mathop{\arg\max}\limits_{w}\sum_{s=1}^{M}P(h^s,v^S)$$

为简化计算，我们令$\sum_{h}score(h,v)=Z(v)$，并将似然函数取对数，最终去求

$$\mathop{\arg\min}\limits_w\left[-\log\sum_{s=1}^{M}\frac{1}{Z(v_s)}\exp\left(\sum_{k=1}^{K}\sum_{i=2}^{N}w_kf_k(h_i^s,h_{i-1}^s,v^s,i)\right)\right]$$

我们利用F1-Score来评价模型的性能。

  

\subsection{英文分词}

\subsubsection{词级别的词元切分 Word-level Tokenization}

英语中单词之间是以空格分隔的，因此基于规则的方法就可以很好地解决英文分词问题，无需动用基于学习的方法。

\subsubsection{亚词级别的词元切分 Subword-level Tokenization}

英语等语言的词汇数量通常能够达到数十万，且其中那些共享词根的单词联系紧密，因此词并非一个合适的切分单元；而字母的数量常常只有几十个，也不是一个合适的切分单元。

\begin{formal}

    \textbf{BPE (Byte Pair Encoding)}是一种基于统计的亚词级别的词元切分方法。其思想是不断将语料中出现频率最高的字母组合成一个新的字母，直到达到预设的词汇量。

\end{formal}

  
  

\section{\centering{语法解析}}

  

\subsection{构成式语法}

\subsubsection{基本概念}

\begin{formal}

    \textbf{构成式语法}是从一个根节点开始，通过一系列的规则将其分解为若干个子节点，直到所有节点都是终结符的语法。\\

    其三要素为：

    \begin{itemize}

        \item 语法成分(Constituent)：语法树中的每个节点；

        \item 语法规则(Rule)：将一个节点分解为若干个子节点的规则；

        \item 词典(Lexicon)：将终结符映射为词汇的词典。

    \end{itemize}

    三者关系如下：

    \begin{center}

        \includegraphics[width=.7\textwidth]{figure/constituency_grammar.png}

    \end{center}

\end{formal}

\begin{formal}

    \textbf{上下文相关语法(Context-Sensitive Grammar, CSG)}中语法树的生成与上下文有关，也即其语法规则中$\rightarrow$左边不止单个元素；\\

    \textbf{上下文无关语法(Context-Free Grammar, CFG)}则相反，其语法规则中$\rightarrow$左边只能是单个元素；\\

    \textbf{乔姆斯基范式(Chomsky Normal Form, CNF)}则在CFG的基础上，每条语法规则的右边只能是两个元素，且每条词典规则只能从一个元素生成一个单词，这样便使其语法树为一个二叉树。\\

    \textbf{概率化的上下文无关语法(Probabilistic Context-Free Grammar, PCFG)}则在CNF的基础上，给每条语法规则与词典规则记录一个概率。

\end{formal}

\subsubsection{分析方法}

\begin{formal}

    \textbf{Cocke-Younger-Kasami算法(CYK算法)}是一种自底向上的动态规划算法，用于判断一个句子是否符合某个CNF语法。

    \begin{example}

        以句子\texttt{She eats a fish with a fork}为例，作一阶梯状表格，若某个格子的正下方与对角线方向格子的组合符合语法规则，则此格子也符合语法规则，最后检查根节点是否符合语法规则。

        \begin{center}

            \includegraphics[width=.5\textwidth]{figure/CKY.png}

        \end{center}

        由于一个句子可能存在多种语法树，概率化之后，每个格子中可记录一个概率，如此可以得到概率最大的语法树。

    \end{example}

\end{formal}

  

\subsection{依存式语法}

\subsubsection{基本概念}

\begin{formal}

    \textbf{依存式语法}是通过各个词之间的依存关系来构建语法树的语法，适用于那些语法成分间顺序不重要的语言。

    \begin{center}

        \includegraphics[width=.4\textwidth]{figure/dependency_grammar.png}

    \end{center}

\end{formal}

\subsubsection{分析方法}

\begin{formal}

    \textbf{Shift-Reduce Parser}会将句子中的词按照先后顺序压入栈中，每压一个词入栈，Parser就会在以下操作中选择一个进行：

    \begin{itemize}

        \item \textbf{Shift}：将句子中的下一个词压入栈中；

        \item \textbf{Reduce}：记录栈顶两个词的依存关系，然后将其中的依存词出栈。

    \end{itemize}

    我们将Parser看作一个分类器，其训练同样依赖已经标注好的语料。

\end{formal}

  

\subsection{基于神经网络的语法解析}

\subsubsection{基本概念}

\begin{formal}

    \textbf{词向量}通常是一个300维的向量，其空间上的远近关系能够在一定程度上反映词语之间的语义关系。

\end{formal}

\begin{formal}

    \textbf{分层Softmax(Hierarchical Softmax)}利用Huffman编码的思想，将输出层的$N$分类（$N$为词汇表大小）转化为$N-1$个二分类，词频越高的词，其二分类的路径相对越短，大大减少了计算量。

\end{formal}

\begin{formal}

    \textbf{负采样(Negative Sample)}同样是为了减少庞大的词汇表带来的巨大计算量。每次迭代只计算出很小一部分词的概率，然后最小化这些词的概率。这些词通过噪声分布随机选出，噪声分布为$P(w_i)=\frac{\alpha_i^{3/4}}{\sum_j\alpha_j^{3/4}}$，其中$\alpha_i$为词频。

\end{formal}

  
  

\section{\centering{具体任务}}

  

\subsection{情感分类}

\subsubsection{输入输出}

\begin{formal}

    \textbf{情感分类}是指输入一段短文本，输出情感类别标签的过程。简单的极性一般分为正面、负面、中性三类，而更加具体的分类还能分出更多情感以及强度。

\end{formal}

  

\subsection{文本匹配}

\subsubsection{输入输出}

\begin{formal}

    \textbf{文本匹配}的输入是两段文本，输出为两段文本的匹配程度。

\end{formal}

\subsubsection{应用}

\begin{formal}

    \textbf{问答系统}：将用户的问题与数据库中的标准问题进行匹配，从而找出答案；

    \textbf{阅读理解}：将选项与问题和文章进行匹配，从而找出答案；

    \textbf{信息检索}：将用户的查询与数据库中的文档进行匹配，从而找出最相关的文档。

\end{formal}

\subsubsection{实现方法}

将两段文本投入编码器，比较这两段文本编码的匹配程度。